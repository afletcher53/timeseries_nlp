{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f670b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas\n",
    "import tqdm\n",
    "import pickle\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0832418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run constants.py\n",
    "tf.random.set_seed(\n",
    "    SEED\n",
    ")\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d78435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datafile():\n",
    "    if not exists(DATA_FILEPATH):\n",
    "        raise ValueError(\"No datafile supplied.\")\n",
    "\n",
    "    for _ in tqdm.tqdm(range(0, 100), ncols=100, desc=\"Loading data..\"):\n",
    "        df = pd.read_csv(DATA_FILEPATH, delimiter=\"\\t\", encoding=\"latin-1\")\n",
    "    print(f\"------Loading {DATA_FILEPATH} is completed ------\")\n",
    "\n",
    "    doy = []  # Calc the day of the year for each entry in file\n",
    "    for index in range(len(df)):\n",
    "        d1 = datetime.strptime(df.iloc[index].date, \"%Y-%m-%d %H:%M:%S\")\n",
    "        day_of_year = d1.timetuple().tm_yday  # returns 1 for January 1st\n",
    "        doy.append(day_of_year)\n",
    "    df[\"day_of_year\"] = doy\n",
    "\n",
    "    print(f\"Total EHRs: {len(df.index)}\")\n",
    "    print(f\"Average EHR character length: {df.ehr.str.split().apply(len).mean()}\")\n",
    "    return df\n",
    "\n",
    "def refactor_dataframe(df):\n",
    "    # New dataframe to hold changed data shape - want to have columns equal to every day of the year, with each row indicating a specific patient. EHR entries are located in each cell\n",
    "    doy = list(range(0, 365))  # Unsuprisingly, there are 365 days in a year\n",
    "    ts_df = pd.DataFrame(\n",
    "            columns=doy\n",
    "        )  # add 365 day of year columns to the new dataframe\n",
    "    max_patient_num: int = len(\n",
    "            df.index\n",
    "        )  # Assumption is that this is Z set i.e. {0, ..., 365}\n",
    "    for i in tqdm.tqdm(range(max_patient_num), desc=\"Rearranging patient data\"):\n",
    "            rows = df.loc[df.patient_id == i]\n",
    "            for index, row in rows.iterrows():\n",
    "                ts_df.at[i, row.day_of_year] = row.ehr\n",
    "    print(\"------ Patient data restructuring is completed ------\")\n",
    "    ts_df.to_csv(REARRANGED_DATA_FILEPATH, index=False)\n",
    "\n",
    "    time_series_df = pd.read_csv(REARRANGED_DATA_FILEPATH)\n",
    "    return time_series_df\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "    no_uppercased = tf.strings.lower(input_string, encoding='utf-8')\n",
    "    no_stars = tf.strings.regex_replace(no_uppercased, \"\\*\", \" \")\n",
    "    no_repeats = tf.strings.regex_replace(no_stars, \"devamını oku\", \"\")    \n",
    "    no_html = tf.strings.regex_replace(no_repeats, \"<br />\", \"\")\n",
    "    no_digits = tf.strings.regex_replace(no_html, \"\\w*\\d\\w*\",\"\")\n",
    "    no_punctuations = tf.strings.regex_replace(no_digits, f\"([{string.punctuation}])\", r\" \")\n",
    "\n",
    "    return no_punctuations\n",
    "    \n",
    "\n",
    "def clamp(minimum: int, x: int, maximum: int):\n",
    "    \"\"\"Clamps an integer between a min/max\"\"\"\n",
    "    return max(minimum, min(x, maximum))\n",
    "\n",
    "\n",
    "\n",
    "class WindowGenerator:\n",
    "    \"\"\"\n",
    "    Class to generate timestep'd data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_width: int, output_width: int, save_windows: bool):\n",
    "        \"\"\"Init Parmas\n",
    "        Args:\n",
    "            input_width (int): The timesteps forming the input sequence\n",
    "            output_width (int): The timesteps forming the output sequence\n",
    "        \"\"\"\n",
    "        self.input_width: int = input_width\n",
    "        self.output_width: int = output_width\n",
    "        self.total_window_size: int = input_width + output_width\n",
    "        self.minimum_day_of_year: int = 0\n",
    "        self.maximum_day_of_year: int = 365\n",
    "        self.save_windows: bool = save_windows\n",
    "\n",
    "    def window_datafile(\n",
    "        self, data: pandas.DataFrame\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        data = data.head(100)\n",
    "        sequence: list = []\n",
    "        labels: list = []\n",
    "        # non_null_indexes = list(\n",
    "        #     zip(*np.where(data.notnull()))\n",
    "        # )  # Get indexes of df where values which are not null\n",
    "\n",
    "        for index, row in data.iterrows():\n",
    "            for column in row.index[row.notnull()]:\n",
    "                column = int(column)\n",
    "                lower_bound = clamp(\n",
    "                    self.minimum_day_of_year,\n",
    "                    column - self.input_width,\n",
    "                    self.maximum_day_of_year,\n",
    "                )\n",
    "\n",
    "                upper_bound = clamp(\n",
    "                    0,\n",
    "                    column + self.input_width,\n",
    "                    self.maximum_day_of_year,\n",
    "                )\n",
    "\n",
    "                visit_index = column + 1\n",
    "\n",
    "                input_sequence = data.iloc[index, lower_bound + 1 : visit_index]\n",
    "                input_sequence = input_sequence.to_numpy()\n",
    "                test_input = input_sequence\n",
    "\n",
    "                output_sequence = data.iloc[index, visit_index : upper_bound + 1]\n",
    "                output_sequence = output_sequence.to_numpy()\n",
    "                if len(input_sequence) < self.input_width:\n",
    "                    input_sequence = self._pad_timeseries(sequence=input_sequence)\n",
    "                if len(input_sequence) != TIME_STEP:\n",
    "                    raise ValueError(\n",
    "                        f\"Input sequence has incorrect length :{len(input_sequence)} when compared to timestep window: {TIME_STEP -1}\"\n",
    "                    )\n",
    "                sequence.append(input_sequence)\n",
    "\n",
    "                label = self._categorize_output_sequence(\n",
    "                    output_sequence=output_sequence\n",
    "                )\n",
    "                labels.append(label)\n",
    "        if self.save_windows:\n",
    "            self.save_frames(output_labels=np.array(labels), input_sequence=sequence)\n",
    "\n",
    "        return sequence, np.array(labels)\n",
    "\n",
    "    def _pad_timeseries(self, sequence):\n",
    "        pad_nan_delta = self.input_width - len(sequence)\n",
    "        if pad_nan_delta > 0:\n",
    "            sequence = np.pad(\n",
    "                sequence,\n",
    "                (pad_nan_delta, 0),\n",
    "                \"constant\",\n",
    "                constant_values=EMPTY_TIMESTEP_TOKEN,\n",
    "            )\n",
    "        return sequence\n",
    "\n",
    "    def save_frames(self, output_labels, input_sequence):\n",
    "        print(\"------Saving windows for reuse ------\")\n",
    "        with open(REARRANGED_INPUT_WINDOWED_DATA_FILEPATH, \"wb\") as f:\n",
    "            pickle.dump(input_sequence, f)\n",
    "        with open(REARRANGED_INPUT_WINDOWED_LABEL_FILEPATH, \"wb\") as f:\n",
    "            pickle.dump(output_labels, f)\n",
    "\n",
    "    def _categorize_output_sequence(self, output_sequence: pandas.DataFrame) -> bool:\n",
    "        \"\"\"Categorise output sequence to binary\n",
    "        Classification is based on if output sequence is not null in the output width\n",
    "        Args:\n",
    "            output_sequence (pandas.DataFrame): Sequence to classify\n",
    "        Returns:\n",
    "            bool: 0 = no revisit, 1 = revisit\n",
    "        \"\"\"\n",
    "        try:\n",
    "            np.isnan(np.sum(output_sequence))\n",
    "            return 0\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "\n",
    "def generate_windows(time_series_df):\n",
    "\n",
    "    w1 = WindowGenerator(\n",
    "                input_width=TIME_STEP, output_width=TIME_STEP, save_windows=True\n",
    "            )\n",
    "    loaded_dataset, loaded_labels = w1.window_datafile(time_series_df)\n",
    "   \n",
    "    print(\"------ Windowed Data Loaded ------\")\n",
    "    return loaded_dataset, loaded_labels\n",
    "\n",
    "\n",
    "def vectorize_data_multi_timestep(text_vectorization, loaded_dataset):\n",
    "    arr = numpy.array(loaded_dataset)\n",
    "    arr[pd.isnull(arr)] = EMPTY_TIMESTEP_TOKEN\n",
    "    input_samples = []\n",
    "    for _, item in enumerate(\n",
    "        tqdm.tqdm(arr, desc=\"Vectoring multi timestep\"),\n",
    "    ):\n",
    "        time_seq = []\n",
    "        for _, timestep in enumerate(item):\n",
    "            time_seq.append(text_vectorization(timestep))\n",
    "        input_samples.append(time_seq)\n",
    "    test = numpy.array(input_samples)\n",
    "    return test\n",
    "\n",
    "def embed_vectors(text_vectorization):\n",
    "    embeddings_index = {}\n",
    "\n",
    "    f = open(GLOVE_300D_FILEPATH)\n",
    "    for line in tqdm.tqdm(f, ncols=100, desc=\"Loading Glove Embeddings.\"):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = numpy.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print(f\"Found {len(embeddings_index)} word vectors.\")\n",
    "\n",
    "    vocabulary = text_vectorization.get_vocabulary()\n",
    "    word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    embedding_matrix = numpy.zeros((MAX_VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "    for word, i in tqdm.tqdm(word_index.items(), desc=\"Embedding Matrix.\"):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bfdf8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data..: 100%|█████████████████████████████████████████████| 100/100 [00:17<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Loading c:\\Users\\aaron\\Desktop\\Longitudinal\\data\\data.csv is completed ------\n",
      "Total EHRs: 23907\n",
      "Average EHR character length: 99.45509683356339\n"
     ]
    }
   ],
   "source": [
    "df = load_datafile()\n",
    "\n",
    "if not LOAD_FROM_SAVE: \n",
    "    time_series_df = refactor_dataframe(df)\n",
    "    loaded_ds, loaded_labels = generate_windows(time_series_df)\n",
    "else:\n",
    "    with open(REARRANGED_INPUT_WINDOWED_DATA_FILEPATH, \"rb\") as f:\n",
    "        loaded_ds = pickle.load(f)\n",
    "    with open(REARRANGED_INPUT_WINDOWED_LABEL_FILEPATH, \"rb\") as f:\n",
    "        loaded_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b837e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(loaded_ds)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_test, loaded_labels, test_size=TEST_TRAIN_SPLIT, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VALIDATION_SPLIT, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d8dfe1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  ((188, 30), (188,)) \n",
      "Validation data shape:  ((21, 30), (21,)) \n",
      "Testing data shape:  ((53, 30), (53,)) \n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data shape:  {X_train.shape, y_train.shape} \")\n",
    "print(f\"Validation data shape:  {X_val.shape, y_val.shape} \")\n",
    "print(f\"Testing data shape:  {X_test.shape, y_test.shape} \")\n",
    "with open(X_TRAIN_INPUT_SAVE_FILE_PRE_VEC, \"wb\") as f:\n",
    "        pickle.dump(X_train, f)\n",
    "with open(X_TEST_INPUT_SAVE_FILE_PRE_VEC, \"wb\") as f:\n",
    "        pickle.dump(X_test, f)\n",
    "with open(X_VAL_INPUT_SAVE_FILE_PRE_VEC, \"wb\") as f:\n",
    "        pickle.dump(X_val, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33c29a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['macintosh',\n",
       " 'mechanic',\n",
       " 'sized',\n",
       " 'accuracy',\n",
       " 'arch',\n",
       " 'overall',\n",
       " 'funky',\n",
       " 'dar',\n",
       " 'comfortably',\n",
       " 'yacht',\n",
       " 'confused',\n",
       " 'wolfgang',\n",
       " 'rotating',\n",
       " 'quit',\n",
       " 'att',\n",
       " 'eaton',\n",
       " 'recalled',\n",
       " 'mst',\n",
       " 'sergeant',\n",
       " 'stationery',\n",
       " 'bonn',\n",
       " 'dundee',\n",
       " 'writings',\n",
       " 'stein',\n",
       " 'griffin',\n",
       " 'shift',\n",
       " 'boards',\n",
       " 'freshwater',\n",
       " 'cgi',\n",
       " 'chronic',\n",
       " 'candidates',\n",
       " 'psychiatry',\n",
       " 'raid',\n",
       " 'tablespoons',\n",
       " 'webdesign',\n",
       " 'specialised',\n",
       " 'lamar',\n",
       " 'monica',\n",
       " 'earthquake',\n",
       " 'adapter',\n",
       " 'clutter',\n",
       " 'gucci',\n",
       " 'substrate',\n",
       " 'groom',\n",
       " 'rounds',\n",
       " 'mute',\n",
       " 'spongebob',\n",
       " 'height',\n",
       " 'mood',\n",
       " 'viewer',\n",
       " 'sitemap',\n",
       " 'intelligence',\n",
       " 'x',\n",
       " 'mapped',\n",
       " 'paperback',\n",
       " 'zodiac',\n",
       " 'relaxation',\n",
       " 'lows',\n",
       " 'venezuela',\n",
       " 'winamp',\n",
       " 'chairperson',\n",
       " 'sioux',\n",
       " 'graduation',\n",
       " 'constitution',\n",
       " 'ostg',\n",
       " 'freight',\n",
       " 'happier',\n",
       " 'doctrine',\n",
       " 'rendering',\n",
       " 'kev',\n",
       " 'ew',\n",
       " 'ats',\n",
       " 'handbags',\n",
       " 'shelton',\n",
       " 'knights',\n",
       " 'bernstein',\n",
       " 'pope',\n",
       " 'adaptive',\n",
       " 'constant',\n",
       " 'designation',\n",
       " 'annex',\n",
       " 'produced',\n",
       " 'pasture',\n",
       " 'delaware',\n",
       " 'sanity',\n",
       " 'eager',\n",
       " 'pepsi',\n",
       " 'telecommunication',\n",
       " 'mona',\n",
       " 'petroleum',\n",
       " 'liquidation',\n",
       " 'transistor',\n",
       " 'generalized',\n",
       " 'vaccination',\n",
       " 'colorful',\n",
       " 'encouraged',\n",
       " 'exist',\n",
       " 'alteration',\n",
       " 'silica',\n",
       " 'attention',\n",
       " 'obsolete',\n",
       " 'libstdc',\n",
       " 'postcards',\n",
       " 'communism',\n",
       " 'benq',\n",
       " 'imac',\n",
       " 'hud',\n",
       " 'noteworthy',\n",
       " 'haas',\n",
       " 'carpet',\n",
       " 'toyota',\n",
       " 'poses',\n",
       " 'thongs',\n",
       " 'tibetan',\n",
       " 'tv',\n",
       " 'dyke',\n",
       " 'punk',\n",
       " 'amendment',\n",
       " 'coaches',\n",
       " 'thunder',\n",
       " 'marcus',\n",
       " 'subway',\n",
       " 'because',\n",
       " 'malaysian',\n",
       " 'low',\n",
       " 'tits',\n",
       " 'hi',\n",
       " 'atk',\n",
       " 'download',\n",
       " 'sco',\n",
       " 'porsche',\n",
       " 'soar',\n",
       " 'programmer',\n",
       " 'pin',\n",
       " 'paxil',\n",
       " 'physical',\n",
       " 'success',\n",
       " 'renovation',\n",
       " 'spaghetti',\n",
       " 'gulf',\n",
       " 'colorado',\n",
       " 'shaft',\n",
       " 'expedited',\n",
       " 'different',\n",
       " 'suche',\n",
       " 'collisions',\n",
       " 'rebates',\n",
       " 'deposits',\n",
       " 'radiation',\n",
       " 'lakewood',\n",
       " 'rain',\n",
       " 'nightclub',\n",
       " 'so',\n",
       " 'kool',\n",
       " 'torrent',\n",
       " 'chandler',\n",
       " 'diff',\n",
       " 'meyer',\n",
       " 'hatchback',\n",
       " 'wrist',\n",
       " 'if',\n",
       " 'wipe',\n",
       " 'until',\n",
       " 'ethanol',\n",
       " 'bt',\n",
       " 'walking',\n",
       " 'webhosting',\n",
       " 'gypsy',\n",
       " 'sparrow',\n",
       " 'setting',\n",
       " 'ammunition',\n",
       " 'yds',\n",
       " 'declared',\n",
       " 'ignore',\n",
       " 'eritrea',\n",
       " 'chelsea',\n",
       " 'sbc',\n",
       " 'praised',\n",
       " 'representative',\n",
       " 'mao',\n",
       " 'felony',\n",
       " 'immune',\n",
       " 'bots',\n",
       " 'movement',\n",
       " 'form',\n",
       " 'nerves',\n",
       " 'aol',\n",
       " 'rocks',\n",
       " 'annotations',\n",
       " 'jacking',\n",
       " 'gemstone',\n",
       " 'wetland',\n",
       " 'ctr',\n",
       " 'thinks',\n",
       " 'ttf',\n",
       " 'miller',\n",
       " 'miniature',\n",
       " 'enclosed',\n",
       " 'ride',\n",
       " 'conftest',\n",
       " 'digest',\n",
       " 'seeker',\n",
       " 'trip',\n",
       " 'silent',\n",
       " 'practicable',\n",
       " 'reps',\n",
       " 'sanction',\n",
       " 'sandy',\n",
       " 'cooke',\n",
       " 'had',\n",
       " 'dal',\n",
       " 'knight',\n",
       " 'homepage',\n",
       " 'thereon',\n",
       " 'configuring',\n",
       " 'gill',\n",
       " 'dinnerware',\n",
       " 'learners',\n",
       " 'comedy',\n",
       " 'simultaneously',\n",
       " 'karaoke',\n",
       " 'freezers',\n",
       " 'boundary',\n",
       " 'chickens',\n",
       " 'peripheral',\n",
       " 'geophysics',\n",
       " 'representatives',\n",
       " 'associated',\n",
       " 'connection',\n",
       " 'prepaid',\n",
       " 'hurry',\n",
       " 'lm',\n",
       " 'desirable',\n",
       " 'muse',\n",
       " 'signup',\n",
       " 'priorities',\n",
       " 'hike',\n",
       " 'graphical',\n",
       " 'spend',\n",
       " 'gown',\n",
       " 'canoe',\n",
       " 'traveled',\n",
       " 'migrants',\n",
       " 'types',\n",
       " 'jew',\n",
       " 'restrictive',\n",
       " 'nairobi',\n",
       " 'pep',\n",
       " 'cpu',\n",
       " 'thompson',\n",
       " 'tenants',\n",
       " 'grinding',\n",
       " 'kinetics',\n",
       " 'competition',\n",
       " 'borders',\n",
       " 'colon',\n",
       " 'jeanne',\n",
       " 'statutory',\n",
       " 'senses',\n",
       " 'crest',\n",
       " 'fetish',\n",
       " 'momentum',\n",
       " 'lopez',\n",
       " 'freelance',\n",
       " 'heels',\n",
       " 'brushed',\n",
       " 'suffers',\n",
       " 'joined',\n",
       " 'ms',\n",
       " 'giovanni',\n",
       " 'commenting',\n",
       " 'bat',\n",
       " 'hardy',\n",
       " 'oprah',\n",
       " 'cdrw',\n",
       " 'microphone',\n",
       " 'latin',\n",
       " 'rcw',\n",
       " 'readily',\n",
       " 'alien',\n",
       " 'odessa',\n",
       " 'carrot',\n",
       " 'midland',\n",
       " 'arrivals',\n",
       " 'hyatt',\n",
       " 'hotel',\n",
       " 'narrow',\n",
       " 'instruments',\n",
       " 'warrant',\n",
       " 'rewrite',\n",
       " 'creative',\n",
       " 'biol',\n",
       " 'eastman',\n",
       " 'stomach',\n",
       " 'winchester',\n",
       " 'although',\n",
       " 'americas',\n",
       " 'assignments',\n",
       " 'sciences',\n",
       " 'drops',\n",
       " 'giants',\n",
       " 'paintings',\n",
       " 'loading',\n",
       " 'storey',\n",
       " 'europe',\n",
       " 'gw',\n",
       " 'dictionary',\n",
       " 'layer',\n",
       " 'caucus',\n",
       " 'mechanism',\n",
       " 'marlborough',\n",
       " 'inevitable',\n",
       " 'poisoning',\n",
       " 'cayman',\n",
       " 'mfr',\n",
       " 'fbi',\n",
       " 'esd',\n",
       " 'sensations',\n",
       " 'wages',\n",
       " 'code',\n",
       " 'serve',\n",
       " 'firefox',\n",
       " 'extended',\n",
       " 'incidence',\n",
       " 'coat',\n",
       " 'coco',\n",
       " 'contribution',\n",
       " 'shoots',\n",
       " 'memorable',\n",
       " 'backyard',\n",
       " 'casino',\n",
       " 'pantyhose',\n",
       " 'analysed',\n",
       " 'bipolar',\n",
       " 'photographs',\n",
       " 'operates',\n",
       " 'companion',\n",
       " 'wooden',\n",
       " 'examines',\n",
       " 'money',\n",
       " 'nu',\n",
       " 'coyote',\n",
       " 'rainbow',\n",
       " 'oslo',\n",
       " 'surround',\n",
       " 'brunei',\n",
       " 'trendy',\n",
       " 'kicking',\n",
       " 'gets',\n",
       " 'rm',\n",
       " 'prom',\n",
       " 'nicholas',\n",
       " 'assessing',\n",
       " 'apprenticeship',\n",
       " 'chipset',\n",
       " 'rss',\n",
       " 'authorizing',\n",
       " 'altogether',\n",
       " 'what',\n",
       " 'linguistics',\n",
       " 'stroller',\n",
       " 'utp',\n",
       " 'take',\n",
       " 'thermal',\n",
       " 'kn',\n",
       " 'roasted',\n",
       " 'mira',\n",
       " 'prescribed',\n",
       " 'whitewater',\n",
       " 'carey',\n",
       " 'swing',\n",
       " 'urinary',\n",
       " 'reseller',\n",
       " 'celebrity',\n",
       " 'periodically',\n",
       " 'pollution',\n",
       " 'liners',\n",
       " 'guide',\n",
       " 'cellular',\n",
       " 'pavement',\n",
       " 'stoves',\n",
       " 'honest',\n",
       " 'xxxx',\n",
       " 'eugene',\n",
       " 'tidal',\n",
       " 'newsletter',\n",
       " 'mining',\n",
       " 'mysterious',\n",
       " 'teal',\n",
       " 'systematically',\n",
       " 'acrobat',\n",
       " 'must',\n",
       " 'constitutes',\n",
       " 'weird',\n",
       " 'oriented',\n",
       " 'uv',\n",
       " 'msg',\n",
       " 'bio',\n",
       " 'gradual',\n",
       " 'pitcher',\n",
       " 'sent',\n",
       " 'dining',\n",
       " 'bidders',\n",
       " 'magazine',\n",
       " 'insulation',\n",
       " 'anglo',\n",
       " 'gaelic',\n",
       " 'fraction',\n",
       " 'reported',\n",
       " 'blackboard',\n",
       " 'hostels',\n",
       " 'senators',\n",
       " 'progressive',\n",
       " 'wastewater',\n",
       " 'apparent',\n",
       " 'skincare',\n",
       " 'filesize',\n",
       " 'ensuring',\n",
       " 'lively',\n",
       " 'counseling',\n",
       " 'exception',\n",
       " 'freeware',\n",
       " 'soprano',\n",
       " 'indirect',\n",
       " 'diazepam',\n",
       " 'interruption',\n",
       " 'pitt',\n",
       " 'wanted',\n",
       " 'chrome',\n",
       " 'bridge',\n",
       " 'affordable',\n",
       " 'infect',\n",
       " 'pipes',\n",
       " 'tec',\n",
       " 'lisp',\n",
       " 'hourly',\n",
       " 'fashionable',\n",
       " 'manuel',\n",
       " 'kona',\n",
       " 'zipper',\n",
       " 'cited',\n",
       " 'erie',\n",
       " 'complainant',\n",
       " 'groceries',\n",
       " 'gev',\n",
       " 'topology',\n",
       " 'locale',\n",
       " 'lives',\n",
       " 'certain',\n",
       " 'learns',\n",
       " 'maori',\n",
       " 'internal',\n",
       " 'lined',\n",
       " 'consensus',\n",
       " 'reviews',\n",
       " 'studying',\n",
       " 'regimen',\n",
       " 'weigh',\n",
       " 'hitachi',\n",
       " 'conspiracy',\n",
       " 'mortgages',\n",
       " 'dubbed',\n",
       " 'attacking',\n",
       " 'macromedia',\n",
       " 'jupiter',\n",
       " 'thoughts',\n",
       " 'intelligent',\n",
       " 'hacks',\n",
       " 'section',\n",
       " 'covered',\n",
       " 'monsters',\n",
       " 'reflect',\n",
       " 'exploits',\n",
       " 'controller',\n",
       " 'dozens',\n",
       " 'sign',\n",
       " 'wanting',\n",
       " 'milfseeker',\n",
       " 'lac',\n",
       " 'speed',\n",
       " 'calories',\n",
       " 'butterfly',\n",
       " 'surveillance',\n",
       " 'pole',\n",
       " 'ombudsman',\n",
       " 'northeast',\n",
       " 'cornerstone',\n",
       " 'applicability',\n",
       " 'reynolds',\n",
       " 'meal',\n",
       " 'invest',\n",
       " 'collaborate',\n",
       " 'agile',\n",
       " 'georges',\n",
       " 'frontal',\n",
       " 'everyday',\n",
       " 'roswell',\n",
       " 'aired',\n",
       " 'diffusion',\n",
       " 'anguilla',\n",
       " 'shade',\n",
       " 'da',\n",
       " 'whiskey',\n",
       " 'russia',\n",
       " 'holy',\n",
       " 'spatial',\n",
       " 'cdp',\n",
       " 'baggage',\n",
       " 'roadmap',\n",
       " 'throwing',\n",
       " 'illustrator',\n",
       " 'tit',\n",
       " 'pass',\n",
       " 'cf',\n",
       " 'rushed',\n",
       " 'smithsonian',\n",
       " 'predecessor',\n",
       " 'theology',\n",
       " 'roche',\n",
       " 'appealing',\n",
       " 'observation',\n",
       " 'lifted',\n",
       " 'nesting',\n",
       " 'census',\n",
       " 'clement',\n",
       " 'spires',\n",
       " 'earn',\n",
       " 'ministerial',\n",
       " 'xoops',\n",
       " 'telephone',\n",
       " 'napoleon',\n",
       " 'heck',\n",
       " 'worms',\n",
       " 'bbc',\n",
       " 'batting',\n",
       " 'suck',\n",
       " 'purely',\n",
       " 'reasonably',\n",
       " 'implicit',\n",
       " 'tide',\n",
       " 'radial',\n",
       " 'morse',\n",
       " 'fluffy',\n",
       " 'vivo',\n",
       " 'hooded',\n",
       " 'assure',\n",
       " 'cereal',\n",
       " 'abby',\n",
       " 'refinement',\n",
       " 'expectations',\n",
       " 'twelve',\n",
       " 'randall',\n",
       " 'ling',\n",
       " 'casey',\n",
       " 'wagner',\n",
       " 'chamber',\n",
       " 'dupont',\n",
       " 'compromised',\n",
       " 'synthesis',\n",
       " 'forwarded',\n",
       " 'them',\n",
       " 'objective',\n",
       " 'curriculum',\n",
       " 'cyclic',\n",
       " 'veterans',\n",
       " 'dale',\n",
       " 'syntax',\n",
       " 'colts',\n",
       " 'pesticide',\n",
       " 'wave',\n",
       " 'excursion',\n",
       " 'raymond',\n",
       " 'traits',\n",
       " 'motley',\n",
       " 'breakers',\n",
       " 'spc',\n",
       " 'compress',\n",
       " 'lucy',\n",
       " 'reusable',\n",
       " 'acting',\n",
       " 'wal',\n",
       " 'novell',\n",
       " 'ack',\n",
       " 'paintball',\n",
       " 'strongest',\n",
       " 'botany',\n",
       " 'autonomy',\n",
       " 'totals',\n",
       " 'statistically',\n",
       " 'roses',\n",
       " 'wight',\n",
       " 'cruising',\n",
       " 'interracial',\n",
       " 'blessed',\n",
       " 'printing',\n",
       " 'towards',\n",
       " 'starsmerchant',\n",
       " 'scalable',\n",
       " 'bin',\n",
       " 'ships',\n",
       " 'illicit',\n",
       " 'fell',\n",
       " 'hotmail',\n",
       " 'cables',\n",
       " 'lighter',\n",
       " 'unsubscribe',\n",
       " 'mexico',\n",
       " 'frequently',\n",
       " 'guru',\n",
       " 'chips',\n",
       " 'seeks',\n",
       " 'tooth',\n",
       " 'soups',\n",
       " 'lenox',\n",
       " 'brackets',\n",
       " 'cliff',\n",
       " 'spokesman',\n",
       " 'equine',\n",
       " 'geoff',\n",
       " 'exemptions',\n",
       " 'sedona',\n",
       " 'drank',\n",
       " 'automobile',\n",
       " 'integrated',\n",
       " 'imposed',\n",
       " 'unseen',\n",
       " 'stepped',\n",
       " 'lebanese',\n",
       " 'coated',\n",
       " 'mostly',\n",
       " 'exhibition',\n",
       " 'upscale',\n",
       " 'marianne',\n",
       " 'demonstrating',\n",
       " 'umd',\n",
       " 'females',\n",
       " 'ramsey',\n",
       " 'hungry',\n",
       " 'british',\n",
       " 'dmc',\n",
       " 'subunit',\n",
       " 'fusion',\n",
       " 'slowed',\n",
       " 'vertically',\n",
       " 'pussy',\n",
       " 'apartment',\n",
       " 'dom',\n",
       " 'quinn',\n",
       " 'bullshit',\n",
       " 'probation',\n",
       " 'ranging',\n",
       " 'instructors',\n",
       " 'side',\n",
       " 'cooperative',\n",
       " 'apology',\n",
       " 'michelle',\n",
       " 'liberia',\n",
       " 'reputation',\n",
       " 'starters',\n",
       " 'clarendon',\n",
       " 'cajun',\n",
       " 'applet',\n",
       " 'cst',\n",
       " 'hygiene',\n",
       " 'bhutan',\n",
       " 'juices',\n",
       " 'indicating',\n",
       " 'graduated',\n",
       " 'huskies',\n",
       " 'middleware',\n",
       " 'populations',\n",
       " 'dong',\n",
       " 'herbal',\n",
       " 'fuji',\n",
       " 'pa',\n",
       " 'buck',\n",
       " 'gone',\n",
       " 'slice',\n",
       " 'repec',\n",
       " 'copyrights',\n",
       " 'runoff',\n",
       " 'apocalypse',\n",
       " 'luncheon',\n",
       " 'jeremiah',\n",
       " 'attained',\n",
       " 'dermatology',\n",
       " 'principals',\n",
       " 'warwick',\n",
       " 'spacecraft',\n",
       " 'punish',\n",
       " 'banff',\n",
       " 'decisions',\n",
       " 'rue',\n",
       " 'gb',\n",
       " 'callback',\n",
       " 'productivity',\n",
       " 'advocate',\n",
       " 'finished',\n",
       " 'panic',\n",
       " 'graphite',\n",
       " 'polly',\n",
       " 'ticker',\n",
       " 'claus',\n",
       " 'sherwood',\n",
       " 'ifdef',\n",
       " 'torn',\n",
       " 'gains',\n",
       " 'trick',\n",
       " 'leo',\n",
       " 'institution',\n",
       " 'neoplasms',\n",
       " 'bids',\n",
       " 'deal',\n",
       " 'occupation',\n",
       " 'buchanan',\n",
       " 'inquire',\n",
       " 'forge',\n",
       " 'tsunami',\n",
       " 'refurbished',\n",
       " 'verisign',\n",
       " 'spank',\n",
       " 'carpenter',\n",
       " 'levels',\n",
       " 'hu',\n",
       " 'newport',\n",
       " 'angelina',\n",
       " 'u',\n",
       " 'metres',\n",
       " 'hormone',\n",
       " 'bangkok',\n",
       " 'under',\n",
       " 'behavioural',\n",
       " 'collectables',\n",
       " 'butts',\n",
       " 'shout',\n",
       " 'stanton',\n",
       " 'naturals',\n",
       " 'third',\n",
       " 'floating',\n",
       " 'marriages',\n",
       " 'traces',\n",
       " 'midst',\n",
       " 'merchantability',\n",
       " 'demands',\n",
       " 'finishes',\n",
       " 'badges',\n",
       " 'susceptible',\n",
       " 'golden',\n",
       " 'developers',\n",
       " 'meanings',\n",
       " 'stoke',\n",
       " 'seekers',\n",
       " 'promptly',\n",
       " 'incidents',\n",
       " 'child',\n",
       " 'unisex',\n",
       " 'control',\n",
       " 'softly',\n",
       " 'sustained',\n",
       " 'replay',\n",
       " 'mammoth',\n",
       " 'remedies',\n",
       " 'ernest',\n",
       " 'docbook',\n",
       " 'rochester',\n",
       " 'uc',\n",
       " 'nose',\n",
       " 'compressor',\n",
       " 'clair',\n",
       " 'plantation',\n",
       " 'puppets',\n",
       " 'spark',\n",
       " 'panther',\n",
       " 'sk',\n",
       " 'peripherals',\n",
       " 'caution',\n",
       " 'irritation',\n",
       " 'moines',\n",
       " 'talents',\n",
       " 'run',\n",
       " 'sgi',\n",
       " 'peak',\n",
       " 'negotiate',\n",
       " 'lengthy',\n",
       " 'bail',\n",
       " 'owed',\n",
       " 'instantly',\n",
       " 'everybody',\n",
       " 'sgd',\n",
       " 'classified',\n",
       " 'communist',\n",
       " 'atx',\n",
       " 'dedication',\n",
       " 'problems',\n",
       " 'wd',\n",
       " 'influences',\n",
       " 'respects',\n",
       " 'seam',\n",
       " 'mixtures',\n",
       " 'neville',\n",
       " 'unusually',\n",
       " 'raw',\n",
       " 'acct',\n",
       " 'infections',\n",
       " 'jw',\n",
       " 'adventures',\n",
       " 'quotations',\n",
       " 'jo',\n",
       " 'instance',\n",
       " 'asynchronous',\n",
       " 'reads',\n",
       " 'taken',\n",
       " 'limitation',\n",
       " 'forte',\n",
       " 'nox',\n",
       " 'hoc',\n",
       " 'blaze',\n",
       " 'avalanche',\n",
       " 'springs',\n",
       " 'motorcycle',\n",
       " 'flyer',\n",
       " 'violations',\n",
       " 'second',\n",
       " 'daylight',\n",
       " 'grain',\n",
       " 'fitzgerald',\n",
       " 'jh',\n",
       " 'rehabilitation',\n",
       " 'soo',\n",
       " 'vance',\n",
       " 'rafting',\n",
       " 'theft',\n",
       " 'protesters',\n",
       " 'annuity',\n",
       " 'albeit',\n",
       " 'gaming',\n",
       " 'mu',\n",
       " 'adverse',\n",
       " 'impressed',\n",
       " 'ascending',\n",
       " 'thumbnail',\n",
       " 'cone',\n",
       " 'flames',\n",
       " 'basic',\n",
       " 'godfather',\n",
       " 'enlargement',\n",
       " 'motions',\n",
       " 'storm',\n",
       " 'pulled',\n",
       " 'browsing',\n",
       " 'carriage',\n",
       " 'vice',\n",
       " 'encoder',\n",
       " 'sigh',\n",
       " 'entries',\n",
       " 'satisfying',\n",
       " 'broadband',\n",
       " 'booster',\n",
       " 'underway',\n",
       " 'ipswich',\n",
       " 'scarlet',\n",
       " 'afghan',\n",
       " 'condition',\n",
       " 'prescribe',\n",
       " 'electra',\n",
       " 'darwin',\n",
       " 'cac',\n",
       " 'graphic',\n",
       " 'vi',\n",
       " 'frog',\n",
       " 'solution',\n",
       " 'uniformly',\n",
       " 'landfill',\n",
       " 'routinely',\n",
       " 'sensor',\n",
       " 'ruler',\n",
       " 'settlement',\n",
       " 'merger',\n",
       " 'authorised',\n",
       " 'hj',\n",
       " 'solicitor',\n",
       " 'kayak',\n",
       " 'population',\n",
       " 'dos',\n",
       " 'prevents',\n",
       " 'liberty',\n",
       " 'herself',\n",
       " 'genuinely',\n",
       " 'nightly',\n",
       " 'bibliographic',\n",
       " 'acoustic',\n",
       " 'fender',\n",
       " 'absorbing',\n",
       " 'lender',\n",
       " 'debates',\n",
       " 'francaise',\n",
       " 'rum',\n",
       " 'exported',\n",
       " 'subsystem',\n",
       " 'kauai',\n",
       " 'illness',\n",
       " 'daly',\n",
       " 'escapes',\n",
       " 'berkeley',\n",
       " 'lu',\n",
       " 'reeves',\n",
       " 'modify',\n",
       " 'weaker',\n",
       " 'arguably',\n",
       " 'bridgewater',\n",
       " 'turtle',\n",
       " 'replica',\n",
       " 'bond',\n",
       " 'wasted',\n",
       " 'spirits',\n",
       " 'moscow',\n",
       " 'afforded',\n",
       " 'productions',\n",
       " 'spoilers',\n",
       " 'katz',\n",
       " 'malibu',\n",
       " 'losses',\n",
       " 'pixel',\n",
       " 'node',\n",
       " 'gia',\n",
       " 'chancellor',\n",
       " 'yugoslavia',\n",
       " 'pearce',\n",
       " 'carol',\n",
       " 'modifications',\n",
       " 'skin',\n",
       " 'mackenzie',\n",
       " 'boots',\n",
       " 'laughed',\n",
       " 'au',\n",
       " 'roots',\n",
       " 'expressive',\n",
       " 'cooking',\n",
       " 'aladdin',\n",
       " 'webmin',\n",
       " 'premature',\n",
       " 'interact',\n",
       " 'alzheimer',\n",
       " 'moe',\n",
       " 'business',\n",
       " 'listeners',\n",
       " 'telegraph',\n",
       " 'lighthouse',\n",
       " 'abdul',\n",
       " 'granny',\n",
       " 'proteins',\n",
       " 'diameter',\n",
       " 'blocks',\n",
       " 'poems',\n",
       " 'ace',\n",
       " 'cs',\n",
       " 'unexpected',\n",
       " 'acl',\n",
       " 'priced',\n",
       " 'edition',\n",
       " 'prerequisite',\n",
       " 'chandelier',\n",
       " 'tutoring',\n",
       " 'arithmetic',\n",
       " 'dc',\n",
       " 'strain',\n",
       " 'mosaic',\n",
       " 'inspector',\n",
       " 'portuguese',\n",
       " 'braces',\n",
       " 'january',\n",
       " 'pea',\n",
       " 'ryder',\n",
       " 'fishermen',\n",
       " 'medications',\n",
       " 'smile',\n",
       " 'dow',\n",
       " 'intensity',\n",
       " 'seems',\n",
       " 'kissed',\n",
       " 'demolition',\n",
       " 'keeping',\n",
       " 'puzzles',\n",
       " 'bloomfield',\n",
       " 'runway',\n",
       " 'supplementary',\n",
       " 'clarkson',\n",
       " 'hewitt',\n",
       " 'health',\n",
       " 'foreman',\n",
       " 'conquer',\n",
       " 'updating',\n",
       " 'helpers',\n",
       " 'peanuts',\n",
       " 'zeppelin',\n",
       " 'manuals',\n",
       " 'published',\n",
       " 'attendant',\n",
       " 'px',\n",
       " 'conceptual',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shave off the training corpora for fine tuning glove embeddings with it\n",
    "train_corpora = X_train[29].str.split()\n",
    "train_corpora = train_corpora.tolist()\n",
    "flat_list_train_corpora = [x for xs in train_corpora for x in xs]\n",
    "flat_list_train_corpora = list(set(flat_list_train_corpora))\n",
    "flat_list_train_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "485088b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11959"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flat_list_train_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1a126ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67d1d855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectoring multi timestep: 100%|██████████| 188/188 [00:33<00:00,  5.63it/s]\n",
      "Vectoring multi timestep: 100%|██████████| 53/53 [00:09<00:00,  5.83it/s]\n",
      "Vectoring multi timestep: 100%|██████████| 21/21 [00:03<00:00,  5.35it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_textvectorisation(lst):\n",
    "    text_vectorization: TextVectorization = TextVectorization(\n",
    "        output_mode=\"int\",\n",
    "        split=\"whitespace\",\n",
    "        max_tokens=MAX_VOCAB_SIZE,\n",
    "        output_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        standardize=custom_standardization\n",
    "    )\n",
    "    text_vectorization.adapt(lst)\n",
    "    return text_vectorization\n",
    "\n",
    "\n",
    "text_vectorization = create_textvectorisation(flat_list_train_corpora)\n",
    "\n",
    "X_train_vec_ds = vectorize_data_multi_timestep(text_vectorization, X_train)\n",
    "X_test_vec_ds = vectorize_data_multi_timestep(text_vectorization, X_test)\n",
    "X_val_vec_ds = vectorize_data_multi_timestep(text_vectorization, X_val)\n",
    "y_train = numpy.array(y_train)\n",
    "y_test = numpy.array(y_test)\n",
    "y_val = numpy.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d7bf91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Glove Embeddings.: 400000it [00:30, 13066.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Matrix.: 100%|██████████| 10000/10000 [00:00<00:00, 407819.77it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = embed_vectors(text_vectorization)\n",
    "vocab = text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae74d5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  ((188, 30, 200), (188,)) \n",
      "Validation data shape:  ((21, 30, 200), (21,)) \n",
      "Testing data shape:  ((53, 30, 200), (53,)) \n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data shape:  {X_train_vec_ds.shape, y_train.shape} \")\n",
    "print(f\"Validation data shape:  {X_val_vec_ds.shape, y_val.shape} \")\n",
    "print(f\"Testing data shape:  {X_test_vec_ds.shape, y_test.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6413a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(X_TRAIN_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(X_train_vec_ds, f)\n",
    "with open(Y_TRAIN_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_train, f)\n",
    "with open(X_TEST_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(X_test_vec_ds, f)\n",
    "with open(Y_TEST_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_test, f)\n",
    "with open(X_VAL_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(X_val_vec_ds, f)\n",
    "with open(Y_VAL_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_val, f)\n",
    "with open(EMBEDDING_MATRIX_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(embedding_matrix, f)\n",
    "with open(VOCAB_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(vocab, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b32150b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre training GLOVE on the training data..!?!\n",
    "\n",
    "import csv\n",
    "from mittens import Mittens\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def glove2dict(glove_filename):\n",
    "    with open(glove_filename, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
    "                for line in reader}\n",
    "    return embed\n",
    "\n",
    "\n",
    "pre_glove = glove2dict(GLOVE_300D_FILEPATH)\n",
    "\n",
    "sw = list(_stop_words.ENGLISH_STOP_WORDS)\n",
    "\n",
    "train_corpora_for_glove = [token.lower() for token in flat_list_train_corpora if (token.lower() not in sw)]\n",
    "oov = [token for token in flat_list_train_corpora if token not in pre_glove.keys()]\n",
    "\n",
    "\n",
    "corp_vocab = list(set(oov))\n",
    "brown_doc = [' '.join(train_corpora_for_glove)]\n",
    "\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
    "X = cv.fit_transform(brown_doc)\n",
    "Xc = (X.T * X)\n",
    "Xc.setdiag(0)\n",
    "coocc_ar = Xc.toarray()\n",
    "\n",
    "mittens_model = Mittens(n=EMBEDDING_DIM, max_iter=1000)\n",
    "\n",
    "new_embeddings = mittens_model.fit(\n",
    "    coocc_ar,\n",
    "    vocab=corp_vocab,\n",
    "    initial_embedding_dict= pre_glove)\n",
    "\n",
    "newglove = dict(zip(corp_vocab, new_embeddings))\n",
    "f = open(FINE_TUNED_GLOVE_300D_FILEPATH,\"wb\")\n",
    "pickle.dump(newglove, f)\n",
    "f.close()\n",
    "for key, value in newglove.items():\n",
    "     print(key, '->', value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c27f0d10bf20a72477623593b1965213322ce86373ada6211624b45eb1e094e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
