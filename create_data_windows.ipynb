{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f670b17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-23 22:29:49.525654: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-23 22:29:49.525677: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/aaron/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas\n",
    "import tqdm\n",
    "import pickle\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0832418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run constants.py\n",
    "tf.random.set_seed(\n",
    "    SEED\n",
    ")\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d78435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_standardization(input_string):\n",
    "#     \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "#     no_uppercased = tf.strings.lower(input_string, encoding='utf-8')\n",
    "#     no_stars = tf.strings.regex_replace(no_uppercased, \"\\*\", \" \")\n",
    "#     no_repeats = tf.strings.regex_replace(no_stars, \"devamını oku\", \"\")    \n",
    "#     no_html = tf.strings.regex_replace(no_repeats, \"<br />\", \"\")\n",
    "#     no_digits = tf.strings.regex_replace(no_html, \"\\w*\\d\\w*\",\"\")\n",
    "#     no_punctuations = tf.strings.regex_replace(no_digits, f\"([{string.punctuation}])\", r\" \")\n",
    "\n",
    "#     return no_punctuations\n",
    "\n",
    "# original_string = \"</br>The Quick Brown Fox 123 * <BR> ?\"\n",
    "def custom_standardization(input_string) -> str:\n",
    "    demo_string = input_string.lower()\n",
    "    demo_string = re.sub(CLEANR, '', demo_string)\n",
    "    demo_string = re.sub(r'[^\\w]', ' ', demo_string)\n",
    "    demo_string = re.sub(r'\\w*\\d\\w*', ' ', demo_string)\n",
    "    demo_string = re.sub(r'\\[^\\w\\s]', ' ', demo_string)\n",
    "    return demo_string\n",
    "\n",
    "\n",
    "\n",
    "def clamp(minimum: int, x: int, maximum: int):\n",
    "    \"\"\"Clamps an integer between a min/max\"\"\"\n",
    "    return max(minimum, min(x, maximum))\n",
    "\n",
    "\n",
    "\n",
    "class WindowGenerator:\n",
    "    \"\"\"\n",
    "    Class to generate timestep'd data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_width: int, output_width: int, save_windows: bool):\n",
    "        \"\"\"Init Parmas\n",
    "        Args:\n",
    "            input_width (int): The timesteps forming the input sequence\n",
    "            output_width (int): The timesteps forming the output sequence\n",
    "        \"\"\"\n",
    "        self.input_width: int = input_width\n",
    "        self.output_width: int = output_width\n",
    "        self.total_window_size: int = input_width + output_width\n",
    "        self.minimum_day_of_year: int = 0\n",
    "        self.maximum_day_of_year: int = 365\n",
    "        self.save_windows: bool = save_windows\n",
    "\n",
    "    def window_datafile(\n",
    "        self, data: pandas.DataFrame\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        sequence: list = []\n",
    "        labels: list = []\n",
    "        for index, row in data.iterrows():\n",
    "            for column in row.index[row.notnull()]:\n",
    "                column = int(column)\n",
    "                lower_bound = clamp(\n",
    "                    self.minimum_day_of_year,\n",
    "                    column - self.input_width,\n",
    "                    self.maximum_day_of_year,\n",
    "                )\n",
    "\n",
    "                upper_bound = clamp(\n",
    "                    0,\n",
    "                    column + self.input_width,\n",
    "                    self.maximum_day_of_year,\n",
    "                )\n",
    "\n",
    "                visit_index = column \n",
    "\n",
    "                input_sequence = data.iloc[index, lower_bound + 1 : visit_index]\n",
    "                input_sequence = input_sequence.to_numpy()\n",
    "                output_sequence = data.iloc[index, visit_index : upper_bound + 1]\n",
    "                output_sequence = output_sequence.to_numpy()\n",
    "                if len(input_sequence) < self.input_width:\n",
    "                    input_sequence = self._pad_timeseries(sequence=input_sequence)\n",
    "                if len(input_sequence) != TIME_STEP:\n",
    "                    raise ValueError(\n",
    "                        f\"Input sequence has incorrect length :{len(input_sequence)} when compared to timestep window: {TIME_STEP -1}\"\n",
    "                    )\n",
    "                sequence.append(input_sequence)\n",
    "\n",
    "                label = self._categorize_output_sequence(\n",
    "                    output_sequence=output_sequence\n",
    "                )\n",
    "                labels.append(label)\n",
    "        if self.save_windows:\n",
    "            self.save_frames(output_labels=np.array(labels), input_sequence=sequence)\n",
    "\n",
    "        return sequence, np.array(labels)\n",
    "\n",
    "    def _pad_timeseries(self, sequence):\n",
    "        pad_nan_delta = self.input_width - len(sequence)\n",
    "        if pad_nan_delta > 0:\n",
    "            sequence = np.pad(\n",
    "                sequence,\n",
    "                (pad_nan_delta, 0),\n",
    "                \"constant\",\n",
    "                constant_values=EMPTY_TIMESTEP_TOKEN,\n",
    "            )\n",
    "        return sequence\n",
    "\n",
    "    def save_frames(self, output_labels, input_sequence):\n",
    "        print(\"------Saving windows for reuse ------\")\n",
    "        with open(REARRANGED_INPUT_WINDOWED_DATA_FILEPATH, \"wb\") as f:\n",
    "            pickle.dump(input_sequence, f)\n",
    "        with open(REARRANGED_INPUT_WINDOWED_LABEL_FILEPATH, \"wb\") as f:\n",
    "            pickle.dump(output_labels, f)\n",
    "\n",
    "    def _categorize_output_sequence(self, output_sequence: pandas.DataFrame) -> bool:\n",
    "        \"\"\"Categorise output sequence to binary\n",
    "        Classification is based on if output sequence is not null in the output width\n",
    "        Args:\n",
    "            output_sequence (pandas.DataFrame): Sequence to classify\n",
    "        Returns:\n",
    "            bool: 0 = no revisit, 1 = revisit\n",
    "        \"\"\"\n",
    "        try:\n",
    "            np.isnan(np.sum(output_sequence))\n",
    "            return 0\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "\n",
    "def generate_windows(time_series_df):\n",
    "\n",
    "    w1 = WindowGenerator(\n",
    "                input_width=TIME_STEP, output_width=TIME_STEP, save_windows=True\n",
    "            )\n",
    "    loaded_dataset, loaded_labels = w1.window_datafile(time_series_df)\n",
    "   \n",
    "    print(\"------ Windowed Data Loaded ------\")\n",
    "    return loaded_dataset, loaded_labels\n",
    "\n",
    "\n",
    "def vectorize_data_multi_timestep(text_vectorization, loaded_dataset):\n",
    "    arr = numpy.array(loaded_dataset)\n",
    "    arr[pd.isnull(arr)] = EMPTY_TIMESTEP_TOKEN\n",
    "    input_samples = []\n",
    "    for _, item in enumerate(\n",
    "        tqdm.tqdm(arr, desc=\"Vectoring multi timestep\"),\n",
    "    ):\n",
    "        time_seq = []\n",
    "        for _, timestep in enumerate(item):\n",
    "            time_seq.append(text_vectorization(timestep))\n",
    "        input_samples.append(time_seq)\n",
    "    test = numpy.array(input_samples)\n",
    "    return test\n",
    "\n",
    "def embed_vectors(text_vectorization):\n",
    "    embeddings_index = {}\n",
    "\n",
    "    f = open(GLOVE_300D_FILEPATH)\n",
    "    for line in tqdm.tqdm(f, ncols=100, desc=\"Loading Glove Embeddings.\"):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = numpy.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print(f\"Found {len(embeddings_index)} word vectors.\")\n",
    "\n",
    "    vocabulary = text_vectorization.get_vocabulary()\n",
    "    word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    embedding_matrix = numpy.zeros((MAX_VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "    for word, i in tqdm.tqdm(word_index.items(), desc=\"Embedding Matrix.\"):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bfdf8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Saving windows for reuse ------\n",
      "------ Windowed Data Loaded ------\n"
     ]
    }
   ],
   "source": [
    "time_series_df = pd.read_csv(REARRANGED_DATA_FILEPATH)\n",
    "time_series_df = time_series_df.iloc[: , 1:]\n",
    "loaded_ds, loaded_labels = generate_windows(time_series_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b837e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(loaded_ds)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_test, loaded_labels, test_size=TEST_TRAIN_SPLIT, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VALIDATION_SPLIT, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d8dfe1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  ((2465, 30), (2465,)) \n",
      "Validation data shape:  ((274, 30), (274,)) \n",
      "Testing data shape:  ((685, 30), (685,)) \n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data shape:  {X_train.shape, y_train.shape} \")\n",
    "print(f\"Validation data shape:  {X_val.shape, y_val.shape} \")\n",
    "print(f\"Testing data shape:  {X_test.shape, y_test.shape} \")\n",
    "with open(X_TRAIN_INPUT_SAVE_FILE_PRE_VEC, \"wb\") as f:\n",
    "        pickle.dump(X_train, f)\n",
    "with open(X_TEST_INPUT_SAVE_FILE_PRE_VEC, \"wb\") as f:\n",
    "        pickle.dump(X_test, f)\n",
    "with open(X_VAL_INPUT_SAVE_FILE_PRE_VEC, \"wb\") as f:\n",
    "        pickle.dump(X_val, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33c29a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prior',\n",
       " 'HR96,',\n",
       " 'Penny',\n",
       " 'dog',\n",
       " 'anus.',\n",
       " 'preanaesthetic',\n",
       " 'news,',\n",
       " '07:33',\n",
       " 'am1ml',\n",
       " 'Sharpe',\n",
       " 'Cytopoint.',\n",
       " \"Couldn't\",\n",
       " 'Uses',\n",
       " '12.5/125mg',\n",
       " 'Alb',\n",
       " 'Herrero',\n",
       " 'auscultation/palpation.',\n",
       " 'defecits',\n",
       " '9.45am',\n",
       " '168.',\n",
       " 'awrae',\n",
       " 'retch',\n",
       " 'lethargic,',\n",
       " 'monitor-',\n",
       " 'SID',\n",
       " 'Bioch',\n",
       " 'severly',\n",
       " 'worse',\n",
       " 'failure.',\n",
       " 'Main',\n",
       " 'iv,',\n",
       " 'muscled',\n",
       " 'discharege,',\n",
       " 'ow',\n",
       " '4.15pm',\n",
       " 'cPLi',\n",
       " '130',\n",
       " 'resee.',\n",
       " 'ATM.',\n",
       " 'appetite,',\n",
       " 'healead',\n",
       " 'clipped,',\n",
       " 'CBC/lyets:',\n",
       " 'wormer.',\n",
       " 'Itchy.',\n",
       " 'diagnostic.',\n",
       " 'SINCE',\n",
       " 'exudate',\n",
       " 'blnn',\n",
       " 'mucous,',\n",
       " 'incorporating',\n",
       " '18.4',\n",
       " 'Park:',\n",
       " 'SAome',\n",
       " 'Pikarevska',\n",
       " 'Pentoject',\n",
       " 'Milprazon',\n",
       " 'Mottram',\n",
       " '_2',\n",
       " 'vm/dr/co/sn,',\n",
       " '_4.6___kg',\n",
       " 'text',\n",
       " 'organs.',\n",
       " '10g',\n",
       " 'usure',\n",
       " 'sorts,',\n",
       " 'andm',\n",
       " 'Sed',\n",
       " 'acc.',\n",
       " 'BCs',\n",
       " 'microchiping',\n",
       " 'muzzel.',\n",
       " 'dullness',\n",
       " '1kg',\n",
       " 'sterile',\n",
       " 'responsive,mm',\n",
       " 'doubt',\n",
       " 'severe',\n",
       " '</span>banged',\n",
       " '0305.locumvet',\n",
       " 'b/l',\n",
       " 'irritated',\n",
       " 'rested',\n",
       " 'applied;',\n",
       " 'Suspected',\n",
       " 'consent',\n",
       " 'disc??',\n",
       " 'health/med',\n",
       " 'sick',\n",
       " 'Importance-',\n",
       " 'sad',\n",
       " 'spotless',\n",
       " 'Instructions:',\n",
       " 'weaker',\n",
       " 'AMY',\n",
       " 'Emmy',\n",
       " 'coag',\n",
       " 'Mia',\n",
       " 'Otomax/clean',\n",
       " 'Poss',\n",
       " 'Revertor',\n",
       " 'smart',\n",
       " 'meg.',\n",
       " 'adult',\n",
       " 'scunthorpe@medivet.co.uk',\n",
       " 'oid',\n",
       " 'sugar',\n",
       " 'Fibre',\n",
       " 'bloods/x-rays',\n",
       " 'perfect',\n",
       " 'weeeknd',\n",
       " 'Microchipped?',\n",
       " '>25kg(6pk)',\n",
       " 'dehydrated',\n",
       " 'P2',\n",
       " 'Maragh',\n",
       " 'detail.',\n",
       " '24/03/2019',\n",
       " '<b>SKIN</b>',\n",
       " 'an',\n",
       " '2/10',\n",
       " 'flees',\n",
       " 'mouthed',\n",
       " 'marked',\n",
       " 'HOLLAND',\n",
       " 'diagnostic',\n",
       " 'midnight,',\n",
       " 'bile/blood',\n",
       " '(EGR/LD/LC)',\n",
       " 'Crt<2.',\n",
       " 'yesterday,',\n",
       " 'drawer',\n",
       " 'wellboved',\n",
       " 'bathe',\n",
       " 'racer',\n",
       " 'Clay',\n",
       " 'germany',\n",
       " 'l4:a121a01',\n",
       " 'vic',\n",
       " 'Est',\n",
       " 'especially',\n",
       " '057625',\n",
       " '24hrs,',\n",
       " 'NON-temperate,',\n",
       " 'exclusion',\n",
       " 'placed,',\n",
       " '01:20',\n",
       " 'q.',\n",
       " 'fractur',\n",
       " 'cycle',\n",
       " 'skin/s/c/linea',\n",
       " 'VACCINATIONS',\n",
       " '(is',\n",
       " 'ER',\n",
       " '10mL',\n",
       " 'underbite',\n",
       " 'December.',\n",
       " 'international',\n",
       " '21-Mar-2019',\n",
       " 'Discomfort',\n",
       " 'LATER',\n",
       " '(MODUFLO)',\n",
       " 'vent',\n",
       " 'regrow.',\n",
       " 'really',\n",
       " 'Tuinea',\n",
       " '19/01/2020',\n",
       " 'VC2:',\n",
       " '75',\n",
       " 'complimentary',\n",
       " 'm/s-',\n",
       " 'RR/T',\n",
       " '12-14',\n",
       " '(USG,',\n",
       " 'only',\n",
       " 'aiming',\n",
       " 'Sugar)',\n",
       " '<b>38.5</b>°C;',\n",
       " '20/06/2019',\n",
       " 'moment:',\n",
       " 'spondylosis',\n",
       " 'painfull',\n",
       " 'RHS.',\n",
       " 'Standardised',\n",
       " 'concerns/vaginal',\n",
       " '(5pk)',\n",
       " 'adj',\n",
       " 'blind,',\n",
       " 'elsewhere,',\n",
       " 'Checkup-Ajg',\n",
       " 'concerend',\n",
       " 'flat',\n",
       " 'blepharospasm,',\n",
       " 'origin,',\n",
       " '10ml,',\n",
       " 'knee.',\n",
       " 'a102f01',\n",
       " 'elicited',\n",
       " 'Dobby',\n",
       " 'disop',\n",
       " 'contents.',\n",
       " 'Monitoring',\n",
       " '19/01/2019',\n",
       " 'referral',\n",
       " 'Chicken',\n",
       " 'demonstated',\n",
       " 'PLEASE',\n",
       " 'lumpy',\n",
       " 'Griffiths',\n",
       " 'RCW.',\n",
       " 'resolved..',\n",
       " '1.035',\n",
       " 'consult',\n",
       " 'interval',\n",
       " 'brain',\n",
       " 'paddling',\n",
       " 'o.',\n",
       " 'plexus',\n",
       " 'hospitaliz,',\n",
       " 'L3-4',\n",
       " '£30',\n",
       " 'feed.',\n",
       " 'ulna/radius.',\n",
       " 'Kit',\n",
       " 'sedate,',\n",
       " 'care-',\n",
       " 'Coco',\n",
       " 'Giovan',\n",
       " 'alwyas.',\n",
       " 'along',\n",
       " 'adrenaline',\n",
       " 'uterine',\n",
       " 'reported.',\n",
       " 'thereafter.',\n",
       " 'caprosyn,',\n",
       " 'DHPL2',\n",
       " 'tympanum.',\n",
       " 'pink/moist,',\n",
       " 'ADV',\n",
       " 'Bravecto,',\n",
       " '13/06/2019',\n",
       " 'post-vaccine',\n",
       " '(data',\n",
       " 'Vac4life/Complete',\n",
       " 'daughters',\n",
       " 'Putting',\n",
       " 'kibble',\n",
       " 'calculus,',\n",
       " 'jh',\n",
       " 'Ovariohysterectomy',\n",
       " 'overwt.',\n",
       " '?',\n",
       " 'anticipated',\n",
       " 'happened',\n",
       " 'crepitic,',\n",
       " 'reqd',\n",
       " 'abnormalites',\n",
       " 'trialled',\n",
       " 'parvo,',\n",
       " 'TAPEWORM</b>',\n",
       " 'resected',\n",
       " 'Thought',\n",
       " 'Pu/Pd.',\n",
       " 'interference',\n",
       " 'administered.',\n",
       " '1.2MLS',\n",
       " 'ProBio',\n",
       " 'canine',\n",
       " 'covered',\n",
       " 'Plan:disp',\n",
       " 'description.',\n",
       " 'Could',\n",
       " 'Nicoll',\n",
       " 'pigmented',\n",
       " 'CE-nad.',\n",
       " 'b/c',\n",
       " 'palaption',\n",
       " 'may.',\n",
       " 'Retained',\n",
       " 'woke',\n",
       " 'Fidget.',\n",
       " 'inflamtion,',\n",
       " 'PRED',\n",
       " 'oa.',\n",
       " 'leeft',\n",
       " '3/0,',\n",
       " 'three',\n",
       " 'shallow,',\n",
       " '09:39',\n",
       " 'general',\n",
       " 'mons',\n",
       " 'premed,',\n",
       " 'Unremarkable',\n",
       " 'f+w.',\n",
       " '3.95%BW',\n",
       " 'kk',\n",
       " 'more?',\n",
       " 'polyphagia.',\n",
       " 'o/n',\n",
       " 'synchronised',\n",
       " 'uploaded',\n",
       " '(75cm)',\n",
       " 'consistent.',\n",
       " '(<1cm',\n",
       " 'aurimic,',\n",
       " 'bath,',\n",
       " 'Hr=110',\n",
       " 'medetomidine,',\n",
       " 'milp.',\n",
       " 'Inj',\n",
       " 'heart&lungs',\n",
       " 'rubbing,',\n",
       " 'Overnight',\n",
       " 'house.',\n",
       " 'distressed',\n",
       " 'S:',\n",
       " 'also',\n",
       " 'Enzymatic',\n",
       " 'night.',\n",
       " 'mg)</span>:',\n",
       " 'sucess',\n",
       " 'consensual',\n",
       " 'flea/',\n",
       " 'NK',\n",
       " 'starve',\n",
       " 'd/t',\n",
       " 'insured.',\n",
       " 'Dear',\n",
       " 'swabs,',\n",
       " 'havinging',\n",
       " 'pericardial',\n",
       " 'CYTO1',\n",
       " 'ivft',\n",
       " '&gt;',\n",
       " 'Maintenance',\n",
       " 'thereof.',\n",
       " 'L2:',\n",
       " 'oxygen.',\n",
       " 'throughout.v',\n",
       " 'CE:QAR',\n",
       " '1.50',\n",
       " 'strangely',\n",
       " 'CKCS',\n",
       " '*IVFT',\n",
       " '2/5,',\n",
       " 'etc..',\n",
       " 'lspa',\n",
       " '90bpm',\n",
       " 'Nicki',\n",
       " '<b>EAR',\n",
       " 'Panniculus',\n",
       " '24-Jan-2019',\n",
       " 'W5Y:',\n",
       " 'recumbecny',\n",
       " 'stomach',\n",
       " '2-3cm',\n",
       " 'regurgitating',\n",
       " 'everyday',\n",
       " 'Burette',\n",
       " 'Lips.',\n",
       " 'commonly',\n",
       " 'Emesis',\n",
       " '<b>ANTIBIOTIC</b>',\n",
       " 'Serum',\n",
       " 'Coleman',\n",
       " 'date',\n",
       " 'fractious',\n",
       " 'excessively',\n",
       " 'soft,',\n",
       " 'was.',\n",
       " 'TN.',\n",
       " 'synchornous',\n",
       " 'frogged',\n",
       " 'neurologically',\n",
       " '(3.7-5.8)',\n",
       " '18:28:01',\n",
       " '3s.',\n",
       " 'Reed',\n",
       " 'bl.',\n",
       " 'hocks',\n",
       " 'Ruptured',\n",
       " '1hr+',\n",
       " 'proceeed',\n",
       " 'UNAUTHORISED',\n",
       " 'PU/PD-,',\n",
       " 'tap.',\n",
       " 'Federica',\n",
       " 'uncomfortable/painfull',\n",
       " 'vbetter',\n",
       " 'Mark',\n",
       " 'SPLIT',\n",
       " 'house',\n",
       " 'pads:',\n",
       " 'A435A01',\n",
       " '24/04/2019',\n",
       " 'L-ear',\n",
       " 'abicve',\n",
       " '£100',\n",
       " '19/03/2019',\n",
       " 'lymphome,',\n",
       " 'met',\n",
       " '0.5mg/ml',\n",
       " '11-2020',\n",
       " 'cuffed,',\n",
       " 'visit:',\n",
       " 'rupture.',\n",
       " 'INFLAMMATORY',\n",
       " 'trnsfer',\n",
       " 'Magrini',\n",
       " 'Eenm',\n",
       " 'POCin',\n",
       " 'lungsseemed',\n",
       " 'A118D01/',\n",
       " '(09-2019)',\n",
       " 'appear',\n",
       " 'EENT',\n",
       " '2-3x',\n",
       " 'bld.',\n",
       " 'low-grade',\n",
       " \"lola's\",\n",
       " '38',\n",
       " '£25',\n",
       " 'n,',\n",
       " 'trimmex,',\n",
       " 'improved',\n",
       " 'CARDISURE',\n",
       " '22/01',\n",
       " 'Booking',\n",
       " 'Trial',\n",
       " 'interrupted)',\n",
       " 'painful)',\n",
       " 'ataxia?',\n",
       " 'glimpse',\n",
       " '(22-30cm)',\n",
       " '(under',\n",
       " '05008253',\n",
       " 'Drain',\n",
       " 'different',\n",
       " '12/08/2019,',\n",
       " 'feew',\n",
       " 'prostate.',\n",
       " 'a123b01',\n",
       " 'Board',\n",
       " 'STAIN/',\n",
       " 'Spleen:',\n",
       " 'Casimir',\n",
       " 'pinpoint',\n",
       " '15/07/2019',\n",
       " 'pets,',\n",
       " 'SO',\n",
       " 'deterioaration',\n",
       " 'flash,',\n",
       " 'April',\n",
       " 'meth+medet',\n",
       " '102,',\n",
       " 'Ro',\n",
       " 'feet.',\n",
       " 'enema',\n",
       " 'sore).',\n",
       " 'furnishings/leather/plastics',\n",
       " 'kidneys',\n",
       " 'held.',\n",
       " 'test:',\n",
       " 'declined,',\n",
       " 'Casey',\n",
       " 'Nutraflora',\n",
       " 'Lewis,',\n",
       " 'yeast/B',\n",
       " 'adviced',\n",
       " 'REMOVED',\n",
       " 'spayed',\n",
       " 'Delia',\n",
       " 'Ketones.',\n",
       " 'Synulox',\n",
       " 'Rang',\n",
       " 'incorrectly',\n",
       " 'Kg',\n",
       " 'Twice',\n",
       " 'Urinalysis',\n",
       " \"'blood\",\n",
       " 'hypoallergrnic',\n",
       " 'considere',\n",
       " 'EXP:',\n",
       " 'Montgomery',\n",
       " 'discussing.',\n",
       " 'Lc',\n",
       " 'afternoon',\n",
       " 'cortavance.',\n",
       " 'miunimally',\n",
       " 'LA:Ao',\n",
       " 'WOuld',\n",
       " 'w/o',\n",
       " 'offer',\n",
       " 'a10a01',\n",
       " 'QUARTER',\n",
       " 'Pain.',\n",
       " 'bitch-',\n",
       " '(ooh)',\n",
       " 'wary',\n",
       " 'sites',\n",
       " 'what',\n",
       " 'lot.',\n",
       " '40ml',\n",
       " '180bpm',\n",
       " 'handled.',\n",
       " 'incongruence',\n",
       " 'awa',\n",
       " 'a118d01,',\n",
       " 'Night.',\n",
       " 'Bright.',\n",
       " '2%',\n",
       " 'eyes/ears/nose-ok',\n",
       " '(pk20)',\n",
       " 'MVB,',\n",
       " 'RBC',\n",
       " 'VDS',\n",
       " 'active,',\n",
       " 'Laser',\n",
       " 'ectoparasites',\n",
       " 'break',\n",
       " 'ss:',\n",
       " '(3)',\n",
       " 'forsepts',\n",
       " '16:40:17',\n",
       " 'adaptil,',\n",
       " 'A123E01',\n",
       " 'discharge/NOT',\n",
       " 'Repositioned',\n",
       " 'transfer',\n",
       " 'etc-',\n",
       " 'biopsy).',\n",
       " 'vaccine-',\n",
       " '(warm',\n",
       " 'trims.',\n",
       " 'otoscopic',\n",
       " 'Medium-Pack',\n",
       " 'ravenous,',\n",
       " 'dripped',\n",
       " 'grounds,',\n",
       " 'BMY:',\n",
       " 's/d',\n",
       " 'canthal',\n",
       " 'Black',\n",
       " 'Money',\n",
       " 'brand.',\n",
       " '(cover',\n",
       " '20/08/2019',\n",
       " 'stitching',\n",
       " 'hl.',\n",
       " '70,',\n",
       " 'additonal',\n",
       " '1/4',\n",
       " 'CONDITION',\n",
       " 'whern',\n",
       " 'teeth-tartar',\n",
       " 'reason',\n",
       " 'palpebral',\n",
       " 'uncomfy',\n",
       " 'St',\n",
       " 'lipase',\n",
       " 'plaque/calculi',\n",
       " 'handling.',\n",
       " 'Bolus-',\n",
       " 'antibodies',\n",
       " 'follows:',\n",
       " 'LU2OJF',\n",
       " 'Johanna',\n",
       " 'Medial',\n",
       " '16.8kg',\n",
       " 'Think',\n",
       " 'Number',\n",
       " '08.10.19',\n",
       " 'listening.',\n",
       " 'b',\n",
       " 'AOK.',\n",
       " 'Sentest)',\n",
       " 'DROPS</b>',\n",
       " 'meds',\n",
       " '(0-150)',\n",
       " 'boot.',\n",
       " 'paracetomol',\n",
       " 'tim',\n",
       " 'Flaky',\n",
       " 'resolved.',\n",
       " 'required',\n",
       " 'polips',\n",
       " 'Bu;bus',\n",
       " 'atropine,',\n",
       " 'muzzling',\n",
       " 'NUMBER',\n",
       " 'teeth',\n",
       " 'maxon.',\n",
       " 'Clinisolv',\n",
       " '4-',\n",
       " 'notes.',\n",
       " 'Pump',\n",
       " 'Sioux',\n",
       " 'G/A',\n",
       " 'male',\n",
       " 'el',\n",
       " 'slowly,',\n",
       " 'devon',\n",
       " \"Cs's\",\n",
       " 'Removal',\n",
       " '50x1',\n",
       " 'ectopic',\n",
       " 'GALNDS',\n",
       " 'bandaged.',\n",
       " '(48)',\n",
       " 'Stain',\n",
       " 'back.',\n",
       " '04:18',\n",
       " 'pruritis-cont',\n",
       " '17/07/2019',\n",
       " 'pds,',\n",
       " 'IV',\n",
       " 'preferential',\n",
       " 'wobble',\n",
       " 'Eye',\n",
       " 'persist',\n",
       " 'detnal',\n",
       " 'ISo/O2.',\n",
       " '6pk-Single',\n",
       " 'Mary',\n",
       " 'advocate.',\n",
       " 'Haemathology',\n",
       " 'spot.',\n",
       " 'nutrabio',\n",
       " 'paws,',\n",
       " 'rotationary',\n",
       " 'Vomiting',\n",
       " 'fluorescein',\n",
       " 'intubated,',\n",
       " 'Dong',\n",
       " 'Cortisol',\n",
       " 'gop',\n",
       " 'prompt',\n",
       " 'CF',\n",
       " '(£127.44)',\n",
       " \"month's\",\n",
       " 'teaspoon',\n",
       " 'Date:use',\n",
       " 'signalment',\n",
       " '26-30',\n",
       " 'Sedator/Comfortan',\n",
       " 'SKC',\n",
       " 'GUVS',\n",
       " 'apptm',\n",
       " '(about',\n",
       " 'defaecation,',\n",
       " 'Tofferi',\n",
       " 'Cautious',\n",
       " 'drops)',\n",
       " 'curves',\n",
       " 'incisors',\n",
       " 'isathal',\n",
       " '21/7',\n",
       " 'erself,',\n",
       " 'yelled',\n",
       " 'Attempted',\n",
       " 'chemosis,',\n",
       " 'drumahoe',\n",
       " '2017!',\n",
       " 'abve',\n",
       " 'accoridng',\n",
       " 'Gel',\n",
       " '2-4wks.',\n",
       " 'Parvovirus,',\n",
       " 'case',\n",
       " 'ANOREXIA/NAUSEA/CKD',\n",
       " 'satiety,',\n",
       " 'lactulose',\n",
       " 'skinny',\n",
       " 'Shoe',\n",
       " 'Wl',\n",
       " 'halisosis,',\n",
       " 'Heart',\n",
       " 'butador',\n",
       " 'Medicated',\n",
       " '1l',\n",
       " 'phenobarbitone.',\n",
       " 'builds',\n",
       " '6.5ml',\n",
       " 'surgicryl,',\n",
       " '0.7',\n",
       " 'sequential',\n",
       " 'Crt<',\n",
       " 'rub-scratch',\n",
       " 'drum',\n",
       " 're-stalling',\n",
       " 'pieces',\n",
       " 'PPL',\n",
       " 'NOTES',\n",
       " 'skin.',\n",
       " 'backwards',\n",
       " 'Monthly',\n",
       " 'plus3/0,',\n",
       " 'II',\n",
       " 'curls',\n",
       " 'hx',\n",
       " 'intradermal.',\n",
       " 'when',\n",
       " 'Paper',\n",
       " '900008800602872',\n",
       " 'injury/fracture.',\n",
       " 'antiinflamm',\n",
       " 'nbicely,',\n",
       " 'comofortable',\n",
       " 'acp/0.67ml',\n",
       " '(blue)',\n",
       " 'home;',\n",
       " 'a100a01',\n",
       " 'xu',\n",
       " 'antiinflam',\n",
       " 'Toothcleaner',\n",
       " 'Stabilised',\n",
       " 'protexin)',\n",
       " 'carriage',\n",
       " 'alteration',\n",
       " 'up?',\n",
       " 'po',\n",
       " '(clots',\n",
       " 'Rleg',\n",
       " 'milprazon',\n",
       " '2020.',\n",
       " 'Entire.',\n",
       " 'testing.',\n",
       " 'Lumps/Abnormalities:nothing',\n",
       " 'vcc',\n",
       " 'Auscultation',\n",
       " 'cerenia/probiotic.',\n",
       " 'where',\n",
       " 'syncing',\n",
       " '(cvc)',\n",
       " 'Isabel',\n",
       " 'Marking',\n",
       " 'intentionally!',\n",
       " 'PE.',\n",
       " 'KC/',\n",
       " 'diazpealm',\n",
       " 'later-brighter',\n",
       " 'Corky',\n",
       " 'wimpy',\n",
       " 'NL',\n",
       " 'ecto',\n",
       " 'goe',\n",
       " 'pant-',\n",
       " 'suffering',\n",
       " 'lean',\n",
       " 'LN:wnl.',\n",
       " 'failed',\n",
       " '50D',\n",
       " 'dail',\n",
       " 'instability.',\n",
       " 'Growing',\n",
       " ',no',\n",
       " 'SJ',\n",
       " 'extract',\n",
       " '16/12/2019,',\n",
       " 'Cleanaural',\n",
       " 'consensus',\n",
       " 'skin?,',\n",
       " 'Dragging',\n",
       " 'anaemia',\n",
       " 'Surgery',\n",
       " '08:13',\n",
       " 'nay',\n",
       " 'exercise',\n",
       " '38.4',\n",
       " 'non-bony',\n",
       " 'f&w',\n",
       " 'HR:100,',\n",
       " 'G1/3',\n",
       " 'GIVE',\n",
       " 'Y',\n",
       " 'tending',\n",
       " 'Ashby.',\n",
       " 'strike).',\n",
       " 'Taper',\n",
       " 'HPC6',\n",
       " '3/0',\n",
       " '4.5-10kg',\n",
       " '1204,',\n",
       " 'prescription.',\n",
       " 'Nsf',\n",
       " '7/10th',\n",
       " 'excessively.',\n",
       " 'pet',\n",
       " 'activity.',\n",
       " 'pup5,',\n",
       " 'sharp,',\n",
       " '09/2019',\n",
       " 'request',\n",
       " 'innner',\n",
       " 'INSULIN',\n",
       " 'hameatology',\n",
       " 'NS',\n",
       " '160-',\n",
       " '(31-39)',\n",
       " 'hypers',\n",
       " 'straining?!)',\n",
       " 'Wed',\n",
       " 'Muzzle',\n",
       " 'Discomfort,',\n",
       " '32.1kg,',\n",
       " 'Miley',\n",
       " 'eyebrow',\n",
       " 'rehome',\n",
       " 'content.',\n",
       " 'Container',\n",
       " 'rex',\n",
       " 'ideally',\n",
       " 'born',\n",
       " 'advocate,',\n",
       " 'PLUS',\n",
       " 'Ex',\n",
       " 'M.A.,',\n",
       " 'A118A01+A102B01',\n",
       " 'Buprenorphine',\n",
       " 'lethergic',\n",
       " 'atipamezole',\n",
       " 'Peat',\n",
       " 'feeders',\n",
       " '0085.locumvet',\n",
       " '**Please',\n",
       " '(start',\n",
       " 'MHP',\n",
       " 'time~1-1.5',\n",
       " 'Abi',\n",
       " 'investigation/hosp/',\n",
       " 'days.)',\n",
       " 'AN',\n",
       " 'microscope',\n",
       " 'Suffers',\n",
       " 'gismo',\n",
       " 'buttressed',\n",
       " 'Cotton',\n",
       " 'differnet',\n",
       " 'this).',\n",
       " 'PEWNL',\n",
       " 'moaning',\n",
       " 'Phycical',\n",
       " 'H:',\n",
       " 'segmental',\n",
       " 'discout',\n",
       " 'face):',\n",
       " 'cleaned',\n",
       " 'SATURDAY',\n",
       " 'ml(venaak)',\n",
       " 'eryhema,',\n",
       " 'tue',\n",
       " 'Slide(s)',\n",
       " 'wich',\n",
       " 'Rory,',\n",
       " 'l4:',\n",
       " 'aort',\n",
       " 'Little',\n",
       " 'wnlm',\n",
       " 'appears',\n",
       " '8.6kg',\n",
       " 'West',\n",
       " 'PROBS',\n",
       " 'Linzi.Sergeant',\n",
       " 'OTC',\n",
       " 'stuck.',\n",
       " 'DJD',\n",
       " 'metha',\n",
       " 'proportionate',\n",
       " 'uneven',\n",
       " '(EVERY',\n",
       " 'Amy',\n",
       " 'chemosis',\n",
       " 'CRT<2s,',\n",
       " 'orifice.',\n",
       " 'Padian',\n",
       " 'able',\n",
       " 'eye.',\n",
       " 'scope/CT.',\n",
       " 'Dillondoing',\n",
       " '2/4/19',\n",
       " 'personal',\n",
       " 'receive',\n",
       " 'Thick',\n",
       " 'reconstructed',\n",
       " '13/01/2019',\n",
       " '7.5,',\n",
       " 'hernias',\n",
       " '07805146846',\n",
       " 'discarge',\n",
       " 'Admited',\n",
       " 'cannula',\n",
       " 'VetEnvoy',\n",
       " 'Rom',\n",
       " 'Toe',\n",
       " 'butternut',\n",
       " 'passed',\n",
       " 'company:',\n",
       " 'accordingly',\n",
       " 'O)',\n",
       " 'Sternal',\n",
       " 'GIarida',\n",
       " 'brigh',\n",
       " 'V/D',\n",
       " 'investigate',\n",
       " 'while',\n",
       " '4/6',\n",
       " 'Mouth,',\n",
       " 'Subjective:',\n",
       " 'Lethargy.',\n",
       " 'snuffling.',\n",
       " 'saturday.',\n",
       " '19/12/2019',\n",
       " 'testicle',\n",
       " 'fracture,',\n",
       " 'fracture/luxation',\n",
       " 'serum,',\n",
       " 'CRT&lt;2,',\n",
       " 'mand.',\n",
       " 'Cellulitis/Subcutaneous',\n",
       " '02/01/2019',\n",
       " 'wax',\n",
       " 'display',\n",
       " 'parasite',\n",
       " 'snapped.',\n",
       " 'OC-no',\n",
       " 'E/e/n',\n",
       " 'based',\n",
       " 'full++.',\n",
       " 'messing',\n",
       " 'Age:',\n",
       " 'biocehm',\n",
       " 'each',\n",
       " '(0.8-1.8',\n",
       " 'tender.',\n",
       " 'inflammation/foreign',\n",
       " 'it,',\n",
       " 'A431A01/A120D03',\n",
       " '420',\n",
       " 'previcox',\n",
       " 'specifically',\n",
       " '47kg',\n",
       " 'A504B01.',\n",
       " 'Concerned',\n",
       " '1000',\n",
       " 'sternal/axilla,',\n",
       " 'GCO:',\n",
       " 'Heard',\n",
       " 'FIVE',\n",
       " 'FR',\n",
       " 'STI',\n",
       " 'Serous',\n",
       " 'seg',\n",
       " 'ABD',\n",
       " 'manor',\n",
       " 'noticed...',\n",
       " 'Preds',\n",
       " \"Dougie's\",\n",
       " '6.30am',\n",
       " 'T=',\n",
       " '4.17',\n",
       " 't=38',\n",
       " 'BAR++++',\n",
       " 'date)',\n",
       " 'pain-o',\n",
       " '(sch2)',\n",
       " 'heal.',\n",
       " 'correlation',\n",
       " 'Methadyne',\n",
       " 'short',\n",
       " 'Cellvisc',\n",
       " 'illiac',\n",
       " 'retentive.',\n",
       " 'Presented',\n",
       " 'swellong',\n",
       " 'Ag',\n",
       " 'features',\n",
       " '(25)',\n",
       " 'M/Dose',\n",
       " '(10ml)',\n",
       " '-removed',\n",
       " 'Replacement',\n",
       " 'scraching',\n",
       " 'Suburb.',\n",
       " 'Duck',\n",
       " 'generously',\n",
       " 'fat.',\n",
       " 'Hr120bpm',\n",
       " '£355.77',\n",
       " 'ss',\n",
       " 'deteriorated',\n",
       " 'Tachycardia',\n",
       " '121',\n",
       " '(then',\n",
       " 'Abscess',\n",
       " 'Humphrey',\n",
       " 'o/n.',\n",
       " 'picky',\n",
       " '22/11/2019',\n",
       " '01732',\n",
       " '(APHA)',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shave off the training corpora for fine tuning glove embeddings with it\n",
    "train_corpora = X_train[29].str.split()\n",
    "train_corpora = train_corpora.tolist()\n",
    "flat_list_train_corpora = [x for xs in train_corpora for x in xs]\n",
    "flat_list_train_corpora = list(set(flat_list_train_corpora))\n",
    "flat_list_train_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "485088b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30908"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flat_list_train_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67d1d855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-23 22:29:55.554893: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-23 22:29:55.554983: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-23 22:29:55.555040: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-23 22:29:55.555097: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-23 22:29:55.555152: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-23 22:29:55.555231: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-23 22:29:55.555292: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-23 22:29:55.555450: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-23 22:29:55.556219: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Vectoring multi timestep: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2465/2465 [04:18<00:00,  9.55it/s]\n",
      "Vectoring multi timestep: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 685/685 [01:11<00:00,  9.58it/s]\n",
      "Vectoring multi timestep: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 274/274 [00:28<00:00,  9.54it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_textvectorisation(lst):\n",
    "    text_vectorization: TextVectorization = TextVectorization(\n",
    "        output_mode=\"int\",\n",
    "        split=\"whitespace\",\n",
    "        max_tokens=MAX_VOCAB_SIZE,\n",
    "        output_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        standardize=custom_standardization\n",
    "    )\n",
    "    text_vectorization.adapt(lst)\n",
    "    return text_vectorization\n",
    "\n",
    "\n",
    "text_vectorization = create_textvectorisation(flat_list_train_corpora)\n",
    "\n",
    "X_train_vec_ds = vectorize_data_multi_timestep(text_vectorization, X_train)\n",
    "X_test_vec_ds = vectorize_data_multi_timestep(text_vectorization, X_test)\n",
    "X_val_vec_ds = vectorize_data_multi_timestep(text_vectorization, X_val)\n",
    "y_train = numpy.array(y_train)\n",
    "y_test = numpy.array(y_test)\n",
    "y_val = numpy.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d7bf91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Glove Embeddings.: 400000it [00:14, 26733.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Matrix.: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 489559.85it/s]\n"
     ]
    }
   ],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove*.zip\n",
    "\n",
    "embedding_matrix = embed_vectors(text_vectorization)\n",
    "vocab = text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae74d5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  ((2465, 30, 200), (2465,)) \n",
      "Validation data shape:  ((274, 30, 200), (274,)) \n",
      "Testing data shape:  ((685, 30, 200), (685,)) \n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data shape:  {X_train_vec_ds.shape, y_train.shape} \")\n",
    "print(f\"Validation data shape:  {X_val_vec_ds.shape, y_val.shape} \")\n",
    "print(f\"Testing data shape:  {X_test_vec_ds.shape, y_test.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6413a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(X_TRAIN_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(X_train_vec_ds, f)\n",
    "with open(Y_TRAIN_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_train, f)\n",
    "with open(X_TEST_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(X_test_vec_ds, f)\n",
    "with open(Y_TEST_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_test, f)\n",
    "with open(X_VAL_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(X_val_vec_ds, f)\n",
    "with open(Y_VAL_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_val, f)\n",
    "with open(EMBEDDING_MATRIX_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(embedding_matrix, f)\n",
    "with open(VOCAB_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(vocab, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d139c54c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "TRAIN_CORPORA: str = os.path.join(DATA_DIR, \"train_corpora.pkl\")\n",
    "with open(TRAIN_CORPORA, \"wb\") as f:\n",
    "        pickle.dump(flat_list_train_corpora, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
