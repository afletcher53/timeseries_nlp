{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f670b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas\n",
    "import tqdm\n",
    "import pickle\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0832418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run constants.py\n",
    "tf.random.set_seed(\n",
    "    SEED\n",
    ")\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d78435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clamp(minimum: int, x: int, maximum: int):\n",
    "    \"\"\"Clamps an integer between a min/max\"\"\"\n",
    "    return max(minimum, min(x, maximum))\n",
    "\n",
    "\n",
    "\n",
    "class WindowGenerator:\n",
    "    \"\"\"\n",
    "    Class to generate timestep'd data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_width: int, output_width: int, save_windows: bool):\n",
    "        \"\"\"Init Parmas\n",
    "        Args:\n",
    "            input_width (int): The timesteps forming the input sequence\n",
    "            output_width (int): The timesteps forming the output sequence\n",
    "        \"\"\"\n",
    "        self.input_width: int = input_width\n",
    "        self.output_width: int = output_width\n",
    "        self.total_window_size: int = input_width + output_width\n",
    "        self.minimum_day_of_year: int = 0\n",
    "        self.maximum_day_of_year: int = 365\n",
    "        self.save_windows: bool = save_windows\n",
    "\n",
    "    def window_datafile(\n",
    "        self, data: pandas.DataFrame\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        sequence: list = []\n",
    "        labels: list = []\n",
    "        for index, row in data.iterrows():\n",
    "            for column in row.index[row.notnull()]:\n",
    "                column = int(column)\n",
    "                lower_bound = clamp(\n",
    "                    self.minimum_day_of_year,\n",
    "                    column - self.input_width,\n",
    "                    self.maximum_day_of_year,\n",
    "                )\n",
    "\n",
    "                upper_bound = clamp(\n",
    "                    0,\n",
    "                    column + self.input_width,\n",
    "                    self.maximum_day_of_year,\n",
    "                )\n",
    "\n",
    "                visit_index = column \n",
    "\n",
    "                input_sequence = data.iloc[index, lower_bound + 1 : visit_index]\n",
    "                input_sequence = input_sequence.to_numpy()\n",
    "                output_sequence = data.iloc[index, visit_index : upper_bound + 1]\n",
    "                output_sequence = output_sequence.to_numpy()\n",
    "                if len(input_sequence) < self.input_width:\n",
    "                    input_sequence = self._pad_timeseries(sequence=input_sequence)\n",
    "                if len(input_sequence) != TIME_STEP:\n",
    "                    raise ValueError(\n",
    "                        f\"Input sequence has incorrect length :{len(input_sequence)} when compared to timestep window: {TIME_STEP -1}\"\n",
    "                    )\n",
    "                sequence.append(input_sequence)\n",
    "\n",
    "                label = self._categorize_output_sequence(\n",
    "                    output_sequence=output_sequence\n",
    "                )\n",
    "                labels.append(label)\n",
    "        if self.save_windows:\n",
    "            self.save_frames(output_labels=np.array(labels), input_sequence=sequence)\n",
    "\n",
    "        return sequence, np.array(labels)\n",
    "\n",
    "    def _pad_timeseries(self, sequence):\n",
    "        pad_nan_delta = self.input_width - len(sequence)\n",
    "        if pad_nan_delta > 0:\n",
    "            sequence = np.pad(\n",
    "                sequence,\n",
    "                (pad_nan_delta, 0),\n",
    "                \"constant\",\n",
    "                constant_values=EMPTY_TIMESTEP_TOKEN,\n",
    "            )\n",
    "        return sequence\n",
    "\n",
    "    def save_frames(self, output_labels, input_sequence):\n",
    "        print(\"------Saving windows for reuse ------\")\n",
    "        with open(REARRANGED_INPUT_WINDOWED_DATA_FILEPATH, \"wb\") as f:\n",
    "            pickle.dump(input_sequence, f)\n",
    "        with open(REARRANGED_INPUT_WINDOWED_LABEL_FILEPATH, \"wb\") as f:\n",
    "            pickle.dump(output_labels, f)\n",
    "\n",
    "    def _categorize_output_sequence(self, output_sequence: pandas.DataFrame) -> bool:\n",
    "        \"\"\"Categorise output sequence to binary\n",
    "        Classification is based on if output sequence is not null in the output width\n",
    "        Args:\n",
    "            output_sequence (pandas.DataFrame): Sequence to classify\n",
    "        Returns:\n",
    "            bool: 0 = no revisit, 1 = revisit\n",
    "        \"\"\"\n",
    "        try:\n",
    "            np.isnan(np.sum(output_sequence))\n",
    "            return 0\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "\n",
    "def generate_windows(time_series_df):\n",
    "\n",
    "    w1 = WindowGenerator(\n",
    "                input_width=TIME_STEP, output_width=TIME_STEP, save_windows=True\n",
    "            )\n",
    "    loaded_dataset, loaded_labels = w1.window_datafile(time_series_df)\n",
    "   \n",
    "    print(\"------ Windowed Data Loaded ------\")\n",
    "    return loaded_dataset, loaded_labels\n",
    "\n",
    "\n",
    "def vectorize_data_multi_timestep(text_vectorization, loaded_dataset):\n",
    "    arr = numpy.array(loaded_dataset)\n",
    "    arr[pd.isnull(arr)] = EMPTY_TIMESTEP_TOKEN\n",
    "    input_samples = []\n",
    "    for _, item in enumerate(\n",
    "        tqdm.tqdm(arr, desc=\"Vectoring multi timestep\"),\n",
    "    ):\n",
    "        time_seq = []\n",
    "        for _, timestep in enumerate(item):\n",
    "            time_seq.append(text_vectorization(timestep))\n",
    "        input_samples.append(time_seq)\n",
    "    test = numpy.array(input_samples)\n",
    "    return test\n",
    "\n",
    "def embed_vectors(text_vectorization):\n",
    "    embeddings_index = {}\n",
    "\n",
    "    f = open(GLOVE_300D_FILEPATH)\n",
    "    for line in tqdm.tqdm(f, ncols=100, desc=\"Loading Glove Embeddings.\"):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = numpy.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print(f\"Found {len(embeddings_index)} word vectors.\")\n",
    "\n",
    "    vocabulary = text_vectorization.get_vocabulary()\n",
    "    word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    embedding_matrix = numpy.zeros((MAX_VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "    for word, i in tqdm.tqdm(word_index.items(), desc=\"Embedding Matrix.\"):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bfdf8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df = pd.read_csv(REARRANGED_DATA_FILEPATH)\n",
    "time_series_df = time_series_df.iloc[: , 1:]\n",
    "loaded_ds, loaded_labels = generate_windows(time_series_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(loaded_ds)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_test, loaded_labels, test_size=TEST_TRAIN_SPLIT, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VALIDATION_SPLIT, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8dfe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data shape:  {X_train.shape, y_train.shape} \")\n",
    "print(f\"Validation data shape:  {X_val.shape, y_val.shape} \")\n",
    "print(f\"Testing data shape:  {X_test.shape, y_test.shape} \")\n",
    "with open(X_TRAIN_INPUT_SAVE_FILE_PRE_VEC, \"wb\") as f:\n",
    "        pickle.dump(X_train, f)\n",
    "with open(X_TEST_INPUT_SAVE_FILE_PRE_VEC, \"wb\") as f:\n",
    "        pickle.dump(X_test, f)\n",
    "with open(X_VAL_INPUT_SAVE_FILE_PRE_VEC, \"wb\") as f:\n",
    "        pickle.dump(X_val, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485088b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flat_list_train_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_textvectorisation(lst):\n",
    "    text_vectorization: TextVectorization = TextVectorization(\n",
    "        output_mode=\"int\",\n",
    "        split=\"whitespace\",\n",
    "        max_tokens=MAX_VOCAB_SIZE,\n",
    "        output_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    )\n",
    "    text_vectorization.adapt(lst)\n",
    "    return text_vectorization\n",
    "\n",
    "\n",
    "def clean_df(df):\n",
    "    no_uppercase = df.apply(lambda x: x.astype(str).str.lower()) \n",
    "    no_html = no_uppercase.replace(r'<[^<>]*>', '', regex=True)\n",
    "    no_punctuation = no_html.replace(r'[^\\w]', ' ', regex=True)\n",
    "    no_digits = no_punctuation.replace(r'\\w*\\d\\w*', ' ', regex=True)\n",
    "    return no_digits\n",
    "\n",
    "X_train = clean_df(X_train)\n",
    "X_val = clean_df(X_val)\n",
    "X_test = clean_df(X_test)\n",
    "\n",
    "#Shave off the training corpora for fine tuning glove embeddings with it\n",
    "train_corpora = X_train[29].str.split()\n",
    "train_corpora = train_corpora.tolist()\n",
    "flat_list_train_corpora = [x for xs in train_corpora for x in xs]\n",
    "flat_list_train_corpora = list(set(flat_list_train_corpora))\n",
    "flat_list_train_corpora\n",
    "text_vectorization = create_textvectorisation(flat_list_train_corpora)\n",
    "\n",
    "X_train_vec_ds = vectorize_data_multi_timestep(text_vectorization, X_train)\n",
    "X_test_vec_ds = vectorize_data_multi_timestep(text_vectorization, X_test)\n",
    "X_val_vec_ds = vectorize_data_multi_timestep(text_vectorization, X_val)\n",
    "y_train = numpy.array(y_train)\n",
    "y_test = numpy.array(y_test)\n",
    "y_val = numpy.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7bf91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove*.zip\n",
    "\n",
    "embedding_matrix = embed_vectors(text_vectorization)\n",
    "vocab = text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae74d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data shape:  {X_train_vec_ds.shape, y_train.shape} \")\n",
    "print(f\"Validation data shape:  {X_val_vec_ds.shape, y_val.shape} \")\n",
    "print(f\"Testing data shape:  {X_test_vec_ds.shape, y_test.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6413a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(X_TRAIN_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(X_train_vec_ds, f)\n",
    "with open(Y_TRAIN_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_train, f)\n",
    "with open(X_TEST_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(X_test_vec_ds, f)\n",
    "with open(Y_TEST_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_test, f)\n",
    "with open(X_VAL_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(X_val_vec_ds, f)\n",
    "with open(Y_VAL_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_val, f)\n",
    "with open(EMBEDDING_MATRIX_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(embedding_matrix, f)\n",
    "with open(VOCAB_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(vocab, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CORPORA: str = os.path.join(DATA_DIR, \"train_corpora.pkl\")\n",
    "with open(TRAIN_CORPORA, \"wb\") as f:\n",
    "        pickle.dump(flat_list_train_corpora, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6f07f10bbd904384622e2c81da346bac6398da26490ea76cc729ffb1c8c49fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
