{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data..: 100%|█████████████████████████████████████████████| 100/100 [00:16<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Loading ./data/data.csv is completed ------\n",
      "Total EHRs: 23907\n",
      "Average EHR character length: 770.929560379805\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------Saving windows for reuse ------\n",
      "------ Windowed Data Loaded ------\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Embedding, TextVectorization\n",
    "from functions.LSTMModel import LSTMModel\n",
    "from functions.WindowGenerator import WindowGenerator\n",
    "\n",
    "from constants import (\n",
    "    BATCH_SIZE,\n",
    "    DATA_FILEPATH,\n",
    "    EMBEDDING_DIM,\n",
    "    GLOVE_300D_FILEPATH,\n",
    "    MAX_SEQUENCE_LENGTH,\n",
    "    MAX_VOCAB_SIZE,\n",
    "    NUM_EPOCHS,\n",
    "    REARRANGED_DATA_FILEPATH,\n",
    "    REARRANGED_SINGLE_INPUT_WINDOWED_DATA_FILEPATH,\n",
    "    REARRANGED_SINGLE_INPUT_WINDOWED_LABEL_FILEPATH,\n",
    "    TEST_TRAIN_SPLIT,\n",
    ")\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    \"\"\"\n",
    "    1 - Load datafile\n",
    "    2 - Rearrange datafile if needed, if present skip\n",
    "    3 - Create TextVectorization\n",
    "    4 - Window Data\n",
    "    5 - Section into test/train/split\n",
    "    6 - Embed vectors\n",
    "    7 - Fit and Train Model\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    1. Load Datafile\n",
    "    \"\"\"\n",
    "\n",
    "    if not exists(DATA_FILEPATH):\n",
    "        raise ValueError(\"No datafile supplied.\")\n",
    "\n",
    "    for _ in tqdm(range(0, 100), ncols=100, desc=\"Loading data..\"):\n",
    "        df = pd.read_csv(DATA_FILEPATH, delimiter=\"\\t\", encoding=\"latin-1\")\n",
    "    print(f\"------Loading {DATA_FILEPATH} is completed ------\")\n",
    "\n",
    "    doy = []  # Calc the day of the year for each entry in file\n",
    "    for index in range(len(df)):\n",
    "        d1 = datetime.strptime(df.iloc[index].date, \"%Y-%m-%d %H:%M:%S\")\n",
    "        day_of_year = d1.timetuple().tm_yday  # returns 1 for January 1st\n",
    "        doy.append(day_of_year)\n",
    "    df[\"day_of_year\"] = doy\n",
    "\n",
    "    print(f\"Total EHRs: {len(df.index)}\")\n",
    "    print(f\"Average EHR character length: {df.ehr.apply(len).mean()}\")\n",
    "\n",
    "    \"\"\"\n",
    "    2. Create rearranged datafile, if needed\n",
    "    \"\"\"\n",
    "\n",
    "    # New dataframe to hold changed data shape - want to have columns equal to every day of the year, with each row indicating a specific patient. EHR entries are located in each cell\n",
    "    if not exists(REARRANGED_DATA_FILEPATH):\n",
    "        doy = list(range(0, 365))  # Unsuprisingly, there are 365 days in a year\n",
    "        ts_df = pd.DataFrame(\n",
    "            columns=doy\n",
    "        )  # add 365 day of year columns to the new dataframe\n",
    "        max_patient_num: int = len(\n",
    "            df.index\n",
    "        )  # Assumption is that this is Z set i.e. {0, ..., 365}\n",
    "        for i in tqdm(range(max_patient_num), desc=\"Rearranging patient data\"):\n",
    "            rows = df.loc[df.patient_id == i]\n",
    "            for index, row in rows.iterrows():\n",
    "                ts_df.at[i, row.day_of_year] = row.ehr\n",
    "        print(\"------ Patient data restructuring is completed ------\")\n",
    "        ts_df.to_csv(REARRANGED_DATA_FILEPATH, index=False)\n",
    "\n",
    "    time_series_df = pd.read_csv(REARRANGED_DATA_FILEPATH)\n",
    "\n",
    "    \"\"\"\n",
    "    3. Create TextVectorization object\n",
    "    \"\"\"\n",
    "\n",
    "    X_train_text = df.ehr\n",
    "    text_vectorization: TextVectorization = TextVectorization(\n",
    "        output_mode=\"int\",\n",
    "        split=\"whitespace\",\n",
    "        max_tokens=MAX_VOCAB_SIZE,\n",
    "        output_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    )\n",
    "    text_vectorization.adapt(X_train_text)\n",
    "\n",
    "    \"\"\"\n",
    "    4. Window data with WindowGenerator\n",
    "    \"\"\"\n",
    "\n",
    "    if not exists(REARRANGED_SINGLE_INPUT_WINDOWED_LABEL_FILEPATH) and not exists(\n",
    "        REARRANGED_SINGLE_INPUT_WINDOWED_LABEL_FILEPATH\n",
    "    ):\n",
    "        w1 = WindowGenerator(input_width=30, output_width=30, save_windows=True)\n",
    "        w1.window_multi_input_sequence(time_series_df)\n",
    "    with open(REARRANGED_SINGLE_INPUT_WINDOWED_DATA_FILEPATH, \"rb\") as f:\n",
    "        loaded_dataset = pickle.load(f)\n",
    "\n",
    "    with open(REARRANGED_SINGLE_INPUT_WINDOWED_LABEL_FILEPATH, \"rb\") as f:\n",
    "        loaded_labels = pickle.load(f)\n",
    "\n",
    "    print(\"------ Windowed Data Loaded ------\")\n",
    "\n",
    "    \"\"\"\n",
    "    5. Section Data into test/train/split\n",
    "    \"\"\"\n",
    "\n",
    "    # print(loaded_dataset.shape)\n",
    "    return loaded_dataset, text_vectorization\n",
    "loaded_dataset, tv =main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = numpy.where(pd.isna(loaded_dataset), '', loaded_dataset)\n",
    "\n",
    "all_sequnces = []\n",
    "for i in range(a.shape[0]):\n",
    "    sequence = []\n",
    "    for x in range(a[i].shape[0]):\n",
    "        sequence.append(tv(a[i][x]))\n",
    "    all_sequnces.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_6108\\4046579963.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test = numpy.array(all_sequnces)\n"
     ]
    }
   ],
   "source": [
    "test = numpy.array(all_sequnces)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c00f244b7fa2efe7160a5dce0d00df1483f89b6c73c3118c1ba2bc64b6de1d0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
