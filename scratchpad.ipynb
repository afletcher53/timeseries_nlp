{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "from os.path import exists\n",
    "from datetime import datetime\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Embedding, TextVectorization\n",
    "from functions.LSTMModel import LSTMModel\n",
    "from functions.WindowGenerator import WindowGenerator\n",
    "\n",
    "from constants import (\n",
    "    BATCH_SIZE,\n",
    "    DATA_FILEPATH,\n",
    "    EMBEDDING_DIM,\n",
    "    GLOVE_300D_FILEPATH,\n",
    "    MAX_SEQUENCE_LENGTH,\n",
    "    MAX_VOCAB_SIZE,\n",
    "    NUM_EPOCHS,\n",
    "    REARRANGED_DATA_FILEPATH,\n",
    "    REARRANGED_SINGLE_INPUT_WINDOWED_DATA_FILEPATH,\n",
    "    REARRANGED_SINGLE_INPUT_WINDOWED_LABEL_FILEPATH,\n",
    "    TEST_TRAIN_SPLIT,\n",
    ")\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    \"\"\"\n",
    "    1 - Load datafile\n",
    "    2 - Rearrange datafile if needed, if present skip\n",
    "    3 - Create TextVectorization\n",
    "    4 - Window Data\n",
    "    5 - Section into test/train/split\n",
    "    6 - Embed vectors\n",
    "    7 - Fit and Train Model\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    1. Load Datafile\n",
    "    \"\"\"\n",
    "\n",
    "    if not exists(DATA_FILEPATH):\n",
    "        raise ValueError(\"No datafile supplied.\")\n",
    "\n",
    "    for _ in tqdm(range(0, 100), ncols=100, desc=\"Loading data..\"):\n",
    "        df = pd.read_csv(DATA_FILEPATH, delimiter=\"\\t\", encoding=\"latin-1\")\n",
    "    print(f\"------Loading {DATA_FILEPATH} is completed ------\")\n",
    "\n",
    "    doy = []  # Calc the day of the year for each entry in file\n",
    "    for index in range(len(df)):\n",
    "        d1 = datetime.strptime(df.iloc[index].date, \"%Y-%m-%d %H:%M:%S\")\n",
    "        day_of_year = d1.timetuple().tm_yday  # returns 1 for January 1st\n",
    "        doy.append(day_of_year)\n",
    "    df[\"day_of_year\"] = doy\n",
    "\n",
    "    print(f\"Total EHRs: {len(df.index)}\")\n",
    "    print(f\"Average EHR character length: {df.ehr.apply(len).mean()}\")\n",
    "\n",
    "    \"\"\"\n",
    "    2. Create rearranged datafile, if needed\n",
    "    \"\"\"\n",
    "\n",
    "    # New dataframe to hold changed data shape - want to have columns equal to every day of the year, with each row indicating a specific patient. EHR entries are located in each cell\n",
    "    if not exists(REARRANGED_DATA_FILEPATH):\n",
    "        doy = list(range(0, 365))  # Unsuprisingly, there are 365 days in a year\n",
    "        ts_df = pd.DataFrame(\n",
    "            columns=doy\n",
    "        )  # add 365 day of year columns to the new dataframe\n",
    "        max_patient_num: int = len(\n",
    "            df.index\n",
    "        )  # Assumption is that this is Z set i.e. {0, ..., 365}\n",
    "        for i in tqdm(range(max_patient_num), desc=\"Rearranging patient data\"):\n",
    "            rows = df.loc[df.patient_id == i]\n",
    "            for index, row in rows.iterrows():\n",
    "                ts_df.at[i, row.day_of_year] = row.ehr\n",
    "        print(\"------ Patient data restructuring is completed ------\")\n",
    "        ts_df.to_csv(REARRANGED_DATA_FILEPATH, index=False)\n",
    "\n",
    "    time_series_df = pd.read_csv(REARRANGED_DATA_FILEPATH)\n",
    "\n",
    "    \"\"\"\n",
    "    3. Create TextVectorization object\n",
    "    \"\"\"\n",
    "\n",
    "    X_train_text = df.ehr\n",
    "    text_vectorization: TextVectorization = TextVectorization(\n",
    "        output_mode=\"int\",\n",
    "        split=\"whitespace\",\n",
    "        max_tokens=MAX_VOCAB_SIZE,\n",
    "        output_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        \n",
    "    )\n",
    "    text_vectorization.adapt(X_train_text)\n",
    "\n",
    "    \"\"\"\n",
    "    4. Window data with WindowGenerator\n",
    "    \"\"\"\n",
    "\n",
    "    if not exists(REARRANGED_SINGLE_INPUT_WINDOWED_LABEL_FILEPATH) and not exists(\n",
    "        REARRANGED_SINGLE_INPUT_WINDOWED_LABEL_FILEPATH\n",
    "    ):\n",
    "        w1 = WindowGenerator(input_width=30, output_width=30, save_windows=True)\n",
    "        w1.window_multi_input_sequence(time_series_df)\n",
    "    with open(REARRANGED_SINGLE_INPUT_WINDOWED_DATA_FILEPATH, \"rb\") as f:\n",
    "        loaded_dataset = pickle.load(f)\n",
    "\n",
    "    with open(REARRANGED_SINGLE_INPUT_WINDOWED_LABEL_FILEPATH, \"rb\") as f:\n",
    "        loaded_labels = pickle.load(f)\n",
    "\n",
    "    print(\"------ Windowed Data Loaded ------\")\n",
    "\n",
    "    \"\"\"\n",
    "    5. Section Data into test/train/split\n",
    "    \"\"\"\n",
    "\n",
    "    # print(loaded_dataset.shape)\n",
    "    return loaded_dataset, text_vectorization\n",
    "loaded_dataset, tv =main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "arr = numpy.array(loaded_dataset)\n",
    "arr[pd.isnull(arr)] = '<EMPTY>'\n",
    "input_samples = []\n",
    "for index, item in enumerate(arr):\n",
    "    # print(f\"seqeunce : {index}\")\n",
    "    # print(f\"Seq Len : {len(item)}\")\n",
    "    time_seq = []\n",
    "    for timestep_index, timestep in enumerate(item): \n",
    "        # print(f\"Timestep index: {timestep_index}\")\n",
    "        # print(f\"Timestep Value: {timestep}\")\n",
    "        # print(f\"Timestep Value: {tv(timestep)}\")\n",
    "        time_seq.append(tv(timestep))\n",
    "    input_samples.append(time_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = numpy.array(input_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tv.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open(GLOVE_300D_FILEPATH, encoding=\"UTF-8\")\n",
    "for line in tqdm(f, ncols=100, desc=\"Loading Glove Embeddings.\"):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = numpy.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vocabulary = tv.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "embedding_matrix = numpy.zeros((MAX_VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tqdm(word_index.items(), desc=\"Embedding Matrix.\"):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\"\"\"\n",
    "7. Fit and train models\n",
    "\"\"\"\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "        MAX_VOCAB_SIZE,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer \"embedding_6\" (type Embedding).\n\n'str' object has no attribute 'base_dtype'\n\nCall arguments received:\n  â€¢ inputs=<keras.layers.wrappers.TimeDistributed object at 0x000002D837D25450>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     BATCH_SIZE,\n\u001b[0;32m      5\u001b[0m     DATA_FILEPATH,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     TEST_TRAIN_SPLIT,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m inp \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m30\u001b[39m, MAX_SEQUENCE_LENGTH))    \n\u001b[1;32m---> 18\u001b[0m x \u001b[38;5;241m=\u001b[39m TimeDistributed(\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend.py:1535\u001b[0m, in \u001b[0;36mdtype\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeras.backend.dtype\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;129m@doc_controls\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_generate_docs\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdtype\u001b[39m(x):\n\u001b[0;32m   1508\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns the dtype of a Keras tensor or variable, as a string.\u001b[39;00m\n\u001b[0;32m   1509\u001b[0m \n\u001b[0;32m   1510\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1533\u001b[0m \n\u001b[0;32m   1534\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1535\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_dtype\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
      "\u001b[1;31mAttributeError\u001b[0m: Exception encountered when calling layer \"embedding_6\" (type Embedding).\n\n'str' object has no attribute 'base_dtype'\n\nCall arguments received:\n  â€¢ inputs=<keras.layers.wrappers.TimeDistributed object at 0x000002D837D25450>"
     ]
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Bidirectional, LSTM, Embedding\n",
    "import tensorflow as tf\n",
    "from constants import (\n",
    "    BATCH_SIZE,\n",
    "    DATA_FILEPATH,\n",
    "    EMBEDDING_DIM,\n",
    "    GLOVE_300D_FILEPATH,\n",
    "    MAX_SEQUENCE_LENGTH,\n",
    "    MAX_VOCAB_SIZE,\n",
    "    NUM_EPOCHS,\n",
    "    REARRANGED_DATA_FILEPATH,\n",
    "    REARRANGED_SINGLE_INPUT_WINDOWED_DATA_FILEPATH,\n",
    "    REARRANGED_SINGLE_INPUT_WINDOWED_LABEL_FILEPATH,\n",
    "    TEST_TRAIN_SPLIT,\n",
    ")\n",
    "\n",
    "inp = tf.keras.Input(shape=(30, MAX_SEQUENCE_LENGTH))    \n",
    "x = TimeDistributed(Embedding(200, 300)(x))\n",
    "\n",
    "# embedding_layer = Embedding(\n",
    "#         MAX_VOCAB_SIZE,\n",
    "#         EMBEDDING_DIM,\n",
    "#         input_length=MAX_SEQUENCE_LENGTH,\n",
    "#         trainable=False,\n",
    "#     )\n",
    "\n",
    "# x = TimeDistributed(embedding_layer)(x)\n",
    "\n",
    "# #x1 shape : (batch, article_num, word_num, 50)\n",
    "# x1 = TimeDistributed(Bidirectional(LSTM(50, return_sequences = True)))(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "embedding_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "        x = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "        x = Conv1D(64, 5, activation=\"relu\")(x)\n",
    "        x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "        x = Dense(512, activation=\"relu\")(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(512, activation=\"relu\")(x)\n",
    "        outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "        model = Model(sequence_input, outputs)\n",
    "        return model"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c00f244b7fa2efe7160a5dce0d00df1483f89b6c73c3118c1ba2bc64b6de1d0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
