{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow\n",
    "# from keras.optimizers import SGD\n",
    "from keras import Model\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    TimeDistributed,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Input,\n",
    "    Dense,\n",
    "    Conv1D,\n",
    "    Flatten, \n",
    "    Embedding,\n",
    "    Dropout)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run constants.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Saving varaibles for reuse ------\n",
      "X_train shape: (188, 30, 200)\n",
      "Y_train shape: (188,)\n",
      "X_test shape: (53, 30, 200)\n",
      "Y_test shape: (53,)\n",
      "X_val shape: (21, 30, 200)\n",
      "Y_val shape: (21,)\n",
      "Embedding shape: (10000, 300)\n",
      "Total 0 values: 153\n",
      "Total 1 values: 35\n"
     ]
    }
   ],
   "source": [
    "with open(X_TRAIN_INPUT_SAVE_FILE, \"rb\") as f:\n",
    "        x_train = pickle.load(f)\n",
    "with open(Y_TRAIN_INPUT_SAVE_FILE, \"rb\") as f:\n",
    "        y_train = pickle.load(f)\n",
    "with open(X_TEST_INPUT_SAVE_FILE, \"rb\") as f:\n",
    "        x_test = pickle.load(f)\n",
    "with open(Y_TEST_INPUT_SAVE_FILE, \"rb\") as f:\n",
    "        y_test = pickle.load(f)\n",
    "with open(X_VAL_INPUT_SAVE_FILE, \"rb\") as f:\n",
    "        x_val = pickle.load(f)\n",
    "with open(Y_VAL_INPUT_SAVE_FILE, \"rb\") as f:\n",
    "        y_val = pickle.load(f)\n",
    "with open(EMBEDDING_MATRIX_SAVE_FILE, \"rb\") as f:\n",
    "        embedding_matrix = pickle.load(f)\n",
    "\n",
    "print(\"------Saving varaibles for reuse ------\")\n",
    "print(f\"X_train shape: {x_train.shape}\")\n",
    "print(f\"Y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {x_test.shape}\")\n",
    "print(f\"Y_test shape: {y_test.shape}\")\n",
    "print(f\"X_val shape: {x_val.shape}\")\n",
    "print(f\"Y_val shape: {y_val.shape}\")\n",
    "print(f\"Embedding shape: {embedding_matrix.shape}\")\n",
    "print(f\"Total 0 values: {(y_train == 0).sum()}\")\n",
    "print(f\"Total 1 values: {(y_train == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188, 6000)\n",
      "After OverSampling, the shape of train_X: (306, 6000)\n",
      "After OverSampling, the shape of train_y: (306,)\n",
      "After OverSampling, counts of label '1': 153\n",
      "After OverSampling, counts of label '0': 153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(306, 30, 200)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if BALANCE_DATA:\n",
    "    arr = x_train.reshape(len(x_train), -1)\n",
    "    print(arr.shape)\n",
    "    sm = SMOTE(random_state=SEED)\n",
    "    x_train_bal, y_train = sm.fit_resample(arr, y_train.ravel())\n",
    "    print(f\"After OverSampling, the shape of train_X: {x_train_bal.shape}\")\n",
    "    print(f\"After OverSampling, the shape of train_y: {y_train.shape}\")\n",
    "    print(f\"After OverSampling, counts of label '1': {sum(y_train == 1)}\")\n",
    "    print(f\"After OverSampling, counts of label '0': {sum(y_train == 0)}\")\n",
    "    x_train = numpy.reshape(x_train_bal, (-1, x_train.shape[1], x_train.shape[2]))\n",
    "x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "        MAX_VOCAB_SIZE,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stacked_lstm():\n",
    "    document_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedding_sequences =embedding_layer(document_input)\n",
    "    x = LSTM(12, return_sequences=True)(embedding_sequences)\n",
    "    x = LSTM(12)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    doc_model = Model(document_input, x)\n",
    "    input_docs = Input(\n",
    "                shape=(TIME_STEP, MAX_SEQUENCE_LENGTH), name=\"input_docs\", dtype=\"int32\"\n",
    "            )\n",
    "\n",
    "    x = TimeDistributed(doc_model, name=\"token_embedding_model\")(input_docs)\n",
    "    x = LSTM(12)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(input_docs, outputs)\n",
    "\n",
    "    opt = tensorflow.keras.optimizers.Adam(learning_rate=LR, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "    model.compile(\n",
    "                loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n",
    "            )\n",
    "    model.summary()\n",
    "    model.fit(x_train, y_train,batch_size=BATCH_SIZE,\n",
    "            epochs=NUM_EPOCHS, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm():\n",
    "    document_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedding_sequences =embedding_layer(document_input)\n",
    "    x = LSTM(12)(embedding_sequences)\n",
    "    x = Dropout(0.3)(x)\n",
    "    doc_model = Model(document_input, x)\n",
    "    input_docs = Input(\n",
    "                shape=(TIME_STEP, MAX_SEQUENCE_LENGTH), name=\"input_docs\", dtype=\"int32\"\n",
    "            )\n",
    "\n",
    "    x = TimeDistributed(doc_model, name=\"token_embedding_model\")(input_docs)\n",
    "    x = LSTM(12)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(input_docs, outputs)\n",
    "\n",
    "    opt = tensorflow.keras.optimizers.Adam(learning_rate=LR, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "    model.compile(\n",
    "                loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n",
    "            )\n",
    "    model.summary()\n",
    "    model.fit(x_train, y_train,batch_size=BATCH_SIZE,\n",
    "            epochs=NUM_EPOCHS,validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn():\n",
    "    document_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedding_sequences = embedding_layer(document_input)\n",
    "\n",
    "    x = Conv1D(filters=300, kernel_size=5, padding=\"valid\")(embedding_sequences)\n",
    "    x = Dropout(0.3)(x)\n",
    "    doc_model = Model(document_input, x)\n",
    "\n",
    "    input_docs = Input(\n",
    "            shape=(TIME_STEP, MAX_SEQUENCE_LENGTH), name=\"input_docs\", dtype=\"int32\"\n",
    "        )\n",
    "\n",
    "    x = TimeDistributed(doc_model, name=\"token_embedding_model\")(input_docs)\n",
    "    x = Conv1D(filters=300, kernel_size=5, padding=\"valid\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(input_docs, outputs)\n",
    "\n",
    "    opt = tensorflow.keras.optimizers.Adam(learning_rate=LR, beta_1=0.5, beta_2=0.999)\n",
    "    model.compile(\n",
    "                loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n",
    "            )\n",
    "    model.summary()\n",
    "    model.fit(x_train, y_train,batch_size=BATCH_SIZE,\n",
    "            epochs=NUM_EPOCHS,validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_docs (InputLayer)     [(None, 30, 200)]         0         \n",
      "                                                                 \n",
      " token_embedding_model (Time  (None, 30, 196, 300)     3450300   \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 30, 192, 300)      450300    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 30, 192, 300)      0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 1728000)           0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 1728001   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,628,601\n",
      "Trainable params: 2,628,601\n",
      "Non-trainable params: 3,000,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 5s 346ms/step - loss: 0.0000e+00 - accuracy: 0.4575 - val_loss: 0.0000e+00 - val_accuracy: 0.9048\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.9048\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 2s 313ms/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.9048\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 2s 312ms/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.9048\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.9048\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_docs (InputLayer)     [(None, 30, 200)]         0         \n",
      "                                                                 \n",
      " token_embedding_model (Time  (None, 30, 12)           3016224   \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 12)                1200      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 12)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,017,437\n",
      "Trainable params: 17,437\n",
      "Non-trainable params: 3,000,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.5065WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000018AF18F6E60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "5/5 [==============================] - 5s 412ms/step - loss: 0.0000e+00 - accuracy: 0.5065 - val_loss: 0.0000e+00 - val_accuracy: 0.9048\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 0s 57ms/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.9048\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.9048\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.9048\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 0s 57ms/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.9048\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_docs (InputLayer)     [(None, 30, 200)]         0         \n",
      "                                                                 \n",
      " token_embedding_model (Time  (None, 30, 12)           3015024   \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 12)                1200      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 12)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,016,237\n",
      "Trainable params: 16,237\n",
      "Non-trainable params: 3,000,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 3s 244ms/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.9048\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.9048\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.9048\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.9048\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.9048\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_cnn()\n",
    "run_stacked_lstm()\n",
    "run_lstm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c27f0d10bf20a72477623593b1965213322ce86373ada6211624b45eb1e094e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
