{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas\n",
    "import tqdm\n",
    "import pickle\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run constants.py\n",
    "tf.random.set_seed(\n",
    "    SEED\n",
    ")\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datafile():\n",
    "    if not exists(DATA_FILEPATH):\n",
    "        raise ValueError(\"No datafile supplied.\")\n",
    "\n",
    "    for _ in tqdm.tqdm(range(0, 100), ncols=100, desc=\"Loading data..\"):\n",
    "        df = pd.read_csv(DATA_FILEPATH, delimiter=\"\\t\", encoding=\"latin-1\")\n",
    "    print(f\"------Loading {DATA_FILEPATH} is completed ------\")\n",
    "\n",
    "    doy = []  # Calc the day of the year for each entry in file\n",
    "    for index in range(len(df)):\n",
    "        d1 = datetime.strptime(df.iloc[index].date, \"%Y-%m-%d %H:%M:%S\")\n",
    "        day_of_year = d1.timetuple().tm_yday  # returns 1 for January 1st\n",
    "        doy.append(day_of_year)\n",
    "    df[\"day_of_year\"] = doy\n",
    "\n",
    "    print(f\"Total EHRs: {len(df.index)}\")\n",
    "    print(f\"Average EHR character length: {df.ehr.str.split().apply(len).mean()}\")\n",
    "    return df\n",
    "\n",
    "def refactor_dataframe(df):\n",
    "    # New dataframe to hold changed data shape - want to have columns equal to every day of the year, with each row indicating a specific patient. EHR entries are located in each cell\n",
    "    doy = list(range(0, 365))  # Unsuprisingly, there are 365 days in a year\n",
    "    ts_df = pd.DataFrame(\n",
    "            columns=doy\n",
    "        )  # add 365 day of year columns to the new dataframe\n",
    "    max_patient_num: int = len(\n",
    "            df.index\n",
    "        )  # Assumption is that this is Z set i.e. {0, ..., 365}\n",
    "    for i in tqdm.tqdm(range(max_patient_num), desc=\"Rearranging patient data\"):\n",
    "            rows = df.loc[df.patient_id == i]\n",
    "            for index, row in rows.iterrows():\n",
    "                ts_df.at[i, row.day_of_year] = row.ehr\n",
    "    print(\"------ Patient data restructuring is completed ------\")\n",
    "    ts_df.to_csv(REARRANGED_DATA_FILEPATH, index=False)\n",
    "\n",
    "    time_series_df = pd.read_csv(REARRANGED_DATA_FILEPATH)\n",
    "    return time_series_df\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "    no_uppercased = tf.strings.lower(input_string, encoding='utf-8')\n",
    "    no_stars = tf.strings.regex_replace(no_uppercased, \"\\*\", \" \")\n",
    "    no_repeats = tf.strings.regex_replace(no_stars, \"devamını oku\", \"\")    \n",
    "    no_html = tf.strings.regex_replace(no_repeats, \"<br />\", \"\")\n",
    "    no_digits = tf.strings.regex_replace(no_html, \"\\w*\\d\\w*\",\"\")\n",
    "    no_punctuations = tf.strings.regex_replace(no_digits, f\"([{string.punctuation}])\", r\" \")\n",
    "\n",
    "    return no_punctuations\n",
    "    \n",
    "\n",
    "def clamp(minimum: int, x: int, maximum: int):\n",
    "    \"\"\"Clamps an integer between a min/max\"\"\"\n",
    "    return max(minimum, min(x, maximum))\n",
    "\n",
    "\n",
    "\n",
    "class WindowGenerator:\n",
    "    \"\"\"\n",
    "    Class to generate timestep'd data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_width: int, output_width: int, save_windows: bool):\n",
    "        \"\"\"Init Parmas\n",
    "        Args:\n",
    "            input_width (int): The timesteps forming the input sequence\n",
    "            output_width (int): The timesteps forming the output sequence\n",
    "        \"\"\"\n",
    "        self.input_width: int = input_width\n",
    "        self.output_width: int = output_width\n",
    "        self.total_window_size: int = input_width + output_width\n",
    "        self.minimum_day_of_year: int = 0\n",
    "        self.maximum_day_of_year: int = 365\n",
    "        self.save_windows: bool = save_windows\n",
    "\n",
    "    def window_datafile(\n",
    "        self, data: pandas.DataFrame\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        data = data.head(100)\n",
    "        sequence: list = []\n",
    "        labels: list = []\n",
    "        # non_null_indexes = list(\n",
    "        #     zip(*np.where(data.notnull()))\n",
    "        # )  # Get indexes of df where values which are not null\n",
    "\n",
    "        for index, row in data.iterrows():\n",
    "            for column in row.index[row.notnull()]:\n",
    "                column = int(column)\n",
    "                lower_bound = clamp(\n",
    "                    self.minimum_day_of_year,\n",
    "                    column - self.input_width,\n",
    "                    self.maximum_day_of_year,\n",
    "                )\n",
    "\n",
    "                upper_bound = clamp(\n",
    "                    0,\n",
    "                    column + self.input_width,\n",
    "                    self.maximum_day_of_year,\n",
    "                )\n",
    "\n",
    "                visit_index = column + 1\n",
    "\n",
    "                input_sequence = data.iloc[index, lower_bound + 1 : visit_index]\n",
    "                input_sequence = input_sequence.to_numpy()\n",
    "                test_input = input_sequence\n",
    "\n",
    "                output_sequence = data.iloc[index, visit_index : upper_bound + 1]\n",
    "                output_sequence = output_sequence.to_numpy()\n",
    "                if len(input_sequence) < self.input_width:\n",
    "                    input_sequence = self._pad_timeseries(sequence=input_sequence)\n",
    "                if len(input_sequence) != TIME_STEP:\n",
    "                    raise ValueError(\n",
    "                        f\"Input sequence has incorrect length :{len(input_sequence)} when compared to timestep window: {TIME_STEP -1}\"\n",
    "                    )\n",
    "                sequence.append(input_sequence)\n",
    "\n",
    "                label = self._categorize_output_sequence(\n",
    "                    output_sequence=output_sequence\n",
    "                )\n",
    "                labels.append(label)\n",
    "        if self.save_windows:\n",
    "            self.save_frames(output_labels=np.array(labels), input_sequence=sequence)\n",
    "\n",
    "        return sequence, np.array(labels)\n",
    "\n",
    "    def _pad_timeseries(self, sequence):\n",
    "        pad_nan_delta = self.input_width - len(sequence)\n",
    "        if pad_nan_delta > 0:\n",
    "            sequence = np.pad(\n",
    "                sequence,\n",
    "                (pad_nan_delta, 0),\n",
    "                \"constant\",\n",
    "                constant_values=EMPTY_TIMESTEP_TOKEN,\n",
    "            )\n",
    "        return sequence\n",
    "\n",
    "    def save_frames(self, output_labels, input_sequence):\n",
    "        print(\"------Saving windows for reuse ------\")\n",
    "        with open(REARRANGED_INPUT_WINDOWED_DATA_FILEPATH, \"wb\") as f:\n",
    "            pickle.dump(input_sequence, f)\n",
    "        with open(REARRANGED_INPUT_WINDOWED_LABEL_FILEPATH, \"wb\") as f:\n",
    "            pickle.dump(output_labels, f)\n",
    "\n",
    "    def _categorize_output_sequence(self, output_sequence: pandas.DataFrame) -> bool:\n",
    "        \"\"\"Categorise output sequence to binary\n",
    "        Classification is based on if output sequence is not null in the output width\n",
    "        Args:\n",
    "            output_sequence (pandas.DataFrame): Sequence to classify\n",
    "        Returns:\n",
    "            bool: 0 = no revisit, 1 = revisit\n",
    "        \"\"\"\n",
    "        try:\n",
    "            np.isnan(np.sum(output_sequence))\n",
    "            return 0\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "\n",
    "def generate_windows(time_series_df):\n",
    "\n",
    "    w1 = WindowGenerator(\n",
    "                input_width=TIME_STEP, output_width=TIME_STEP, save_windows=True\n",
    "            )\n",
    "    loaded_dataset, loaded_labels = w1.window_datafile(time_series_df)\n",
    "   \n",
    "    print(\"------ Windowed Data Loaded ------\")\n",
    "    return loaded_dataset, loaded_labels\n",
    "\n",
    "\n",
    "def vectorize_data_multi_timestep(text_vectorization, loaded_dataset):\n",
    "    arr = numpy.array(loaded_dataset)\n",
    "    arr[pd.isnull(arr)] = EMPTY_TIMESTEP_TOKEN\n",
    "    input_samples = []\n",
    "    for _, item in enumerate(\n",
    "        tqdm.tqdm(arr, desc=\"Vectoring multi timestep\"),\n",
    "    ):\n",
    "        time_seq = []\n",
    "        for _, timestep in enumerate(item):\n",
    "            time_seq.append(text_vectorization(timestep))\n",
    "        input_samples.append(time_seq)\n",
    "    test = numpy.array(input_samples)\n",
    "    return test\n",
    "\n",
    "def embed_vectors(text_vectorization):\n",
    "    embeddings_index = {}\n",
    "\n",
    "    f = open(GLOVE_300D_FILEPATH)\n",
    "    for line in tqdm.tqdm(f, ncols=100, desc=\"Loading Glove Embeddings.\"):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = numpy.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print(f\"Found {len(embeddings_index)} word vectors.\")\n",
    "\n",
    "    vocabulary = text_vectorization.get_vocabulary()\n",
    "    word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    embedding_matrix = numpy.zeros((MAX_VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "    for word, i in tqdm.tqdm(word_index.items(), desc=\"Embedding Matrix.\"):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bfdf8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data..: 100%|█████████████████████████████████████████████| 100/100 [00:11<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Loading /home/aaron/timeseries_nlp/data/data.csv is completed ------\n",
      "Total EHRs: 23907\n",
      "Average EHR character length: 99.45509683356339\n"
     ]
    }
   ],
   "source": [
    "df = load_datafile()\n",
    "\n",
    "if not LOAD_FROM_SAVE: \n",
    "    time_series_df = refactor_dataframe(df)\n",
    "    loaded_ds, loaded_labels = generate_windows(time_series_df)\n",
    "else:\n",
    "    with open(REARRANGED_INPUT_WINDOWED_DATA_FILEPATH, \"rb\") as f:\n",
    "        loaded_ds = pickle.load(f)\n",
    "    with open(REARRANGED_INPUT_WINDOWED_LABEL_FILEPATH, \"rb\") as f:\n",
    "        loaded_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(loaded_ds)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_test, loaded_labels, test_size=TEST_TRAIN_SPLIT, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=VALIDATION_SPLIT, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8dfe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data shape:  {X_train.shape, y_train.shape} \")\n",
    "print(f\"Validation data shape:  {X_val.shape, y_val.shape} \")\n",
    "print(f\"Testing data shape:  {X_test.shape, y_test.shape} \")\n",
    "with open(X_TRAIN_INPUT_SAVE_FILE_PRE_VEC, \"wb\") as f:\n",
    "        pickle.dump(X_train, f)\n",
    "with open(X_TEST_INPUT_SAVE_FILE_PRE_VEC, \"wb\") as f:\n",
    "        pickle.dump(X_test, f)\n",
    "with open(X_VAL_INPUT_SAVE_FILE_PRE_VEC, \"wb\") as f:\n",
    "        pickle.dump(X_val, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c29a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shave off the training corpora for fine tuning glove embeddings with it\n",
    "train_corpora = X_train[29].str.split()\n",
    "train_corpora = train_corpora.tolist()\n",
    "flat_list_train_corpora = [x for xs in train_corpora for x in xs]\n",
    "flat_list_train_corpora = list(set(flat_list_train_corpora))\n",
    "flat_list_train_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485088b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flat_list_train_corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a126ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_textvectorisation(lst):\n",
    "    text_vectorization: TextVectorization = TextVectorization(\n",
    "        output_mode=\"int\",\n",
    "        split=\"whitespace\",\n",
    "        max_tokens=MAX_VOCAB_SIZE,\n",
    "        output_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        standardize=custom_standardization\n",
    "    )\n",
    "    text_vectorization.adapt(lst)\n",
    "    return text_vectorization\n",
    "\n",
    "\n",
    "text_vectorization = create_textvectorisation(flat_list_train_corpora)\n",
    "\n",
    "X_train_vec_ds = vectorize_data_multi_timestep(text_vectorization, X_train)\n",
    "X_test_vec_ds = vectorize_data_multi_timestep(text_vectorization, X_test)\n",
    "X_val_vec_ds = vectorize_data_multi_timestep(text_vectorization, X_val)\n",
    "y_train = numpy.array(y_train)\n",
    "y_test = numpy.array(y_test)\n",
    "y_val = numpy.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7bf91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = embed_vectors(text_vectorization)\n",
    "vocab = text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae74d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data shape:  {X_train_vec_ds.shape, y_train.shape} \")\n",
    "print(f\"Validation data shape:  {X_val_vec_ds.shape, y_val.shape} \")\n",
    "print(f\"Testing data shape:  {X_test_vec_ds.shape, y_test.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6413a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(X_TRAIN_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(X_train_vec_ds, f)\n",
    "with open(Y_TRAIN_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_train, f)\n",
    "with open(X_TEST_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(X_test_vec_ds, f)\n",
    "with open(Y_TEST_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_test, f)\n",
    "with open(X_VAL_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(X_val_vec_ds, f)\n",
    "with open(Y_VAL_INPUT_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_val, f)\n",
    "with open(EMBEDDING_MATRIX_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(embedding_matrix, f)\n",
    "with open(VOCAB_SAVE_FILE, \"wb\") as f:\n",
    "        pickle.dump(vocab, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32150b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre training GLOVE on the training data..!?!\n",
    "\n",
    "import csv\n",
    "from mittens import Mittens\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def glove2dict(glove_filename):\n",
    "    with open(glove_filename, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
    "                for line in reader}\n",
    "    return embed\n",
    "\n",
    "\n",
    "pre_glove = glove2dict(GLOVE_300D_FILEPATH)\n",
    "\n",
    "sw = list(_stop_words.ENGLISH_STOP_WORDS)\n",
    "\n",
    "train_corpora_for_glove = [token.lower() for token in flat_list_train_corpora if (token.lower() not in sw)]\n",
    "oov = [token for token in flat_list_train_corpora if token not in pre_glove.keys()]\n",
    "\n",
    "\n",
    "corp_vocab = list(set(oov))\n",
    "brown_doc = [' '.join(train_corpora_for_glove)]\n",
    "\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
    "X = cv.fit_transform(brown_doc)\n",
    "Xc = (X.T * X)\n",
    "Xc.setdiag(0)\n",
    "coocc_ar = Xc.toarray()\n",
    "\n",
    "mittens_model = Mittens(n=EMBEDDING_DIM, max_iter=1000)\n",
    "\n",
    "new_embeddings = mittens_model.fit(\n",
    "    coocc_ar,\n",
    "    vocab=corp_vocab,\n",
    "    initial_embedding_dict= pre_glove)\n",
    "\n",
    "newglove = dict(zip(corp_vocab, new_embeddings))\n",
    "f = open(FINE_TUNED_GLOVE_300D_FILEPATH,\"wb\")\n",
    "pickle.dump(newglove, f)\n",
    "f.close()\n",
    "for key, value in newglove.items():\n",
    "     print(key, '->', value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c27f0d10bf20a72477623593b1965213322ce86373ada6211624b45eb1e094e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
