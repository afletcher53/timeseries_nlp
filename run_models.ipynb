{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 17:47:30.687545: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-15 17:47:30.687569: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/aaron/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow\n",
    "from keras.optimizers import SGD\n",
    "from keras import Model\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    TimeDistributed,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Input,\n",
    "    Dense,\n",
    "    Conv1D,\n",
    "    Flatten, \n",
    "    Embedding)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR:str = '/home/aaron/timeseries_nlp'\n",
    "constants_file = os.path.join(HOME_DIR, 'constants.py')\n",
    "%run '/home/aaron/timeseries_nlp/constants.py'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Saving varaibles for reuse ------\n",
      "X_train shape: (17212, 30, 200)\n",
      "Y_train shape: (17212,)\n",
      "X_test shape: (4782, 30, 200)\n",
      "Y_test shape: (4782,)\n",
      "X_val shape: (1913, 30, 200)\n",
      "Y_val shape: (1913,)\n",
      "Embedding shape: (10000, 300)\n",
      "Total 0 values: 14308\n",
      "Total 1 values: 2904\n"
     ]
    }
   ],
   "source": [
    "with open(X_TRAIN_MULTI_INPUT_SAVE_FILE, \"rb\") as f:\n",
    "        x_train = pickle.load(f)\n",
    "with open(Y_TRAIN_MULTI_INPUT_SAVE_FILE, \"rb\") as f:\n",
    "        y_train = pickle.load(f)\n",
    "with open(X_TEST_MULTI_INPUT_SAVE_FILE, \"rb\") as f:\n",
    "        x_test = pickle.load(f)\n",
    "with open(Y_TEST_MULTI_INPUT_SAVE_FILE, \"rb\") as f:\n",
    "        y_test = pickle.load(f)\n",
    "with open(X_VAL_MULTI_INPUT_SAVE_FILE, \"rb\") as f:\n",
    "        x_val = pickle.load(f)\n",
    "with open(Y_VAL_MULTI_INPUT_SAVE_FILE, \"rb\") as f:\n",
    "        y_val = pickle.load(f)\n",
    "with open(EMBEDDING_MATRIX_SAVE_FILE, \"rb\") as f:\n",
    "        embedding_matrix = pickle.load(f)\n",
    "\n",
    "print(\"------Saving varaibles for reuse ------\")\n",
    "print(f\"X_train shape: {x_train.shape}\")\n",
    "print(f\"Y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {x_test.shape}\")\n",
    "print(f\"Y_test shape: {y_test.shape}\")\n",
    "print(f\"X_val shape: {x_val.shape}\")\n",
    "print(f\"Y_val shape: {y_val.shape}\")\n",
    "print(f\"Embedding shape: {embedding_matrix.shape}\")\n",
    "print(f\"Total 0 values: {(y_train == 0).sum()}\")\n",
    "print(f\"Total 1 values: {(y_train == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17212, 6000)\n",
      "(17212,)\n",
      "After OverSampling, the shape of train_X: (28616, 6000)\n",
      "After OverSampling, the shape of train_y: (28616,)\n",
      "After OverSampling, counts of label '1': 14308\n",
      "After OverSampling, counts of label '0': 14308\n"
     ]
    }
   ],
   "source": [
    "if BALANCE_DATA:\n",
    "    arr = x_train.reshape(len(x_train), -1)\n",
    "    sm = SMOTE(random_state=SEED)\n",
    "    x_train_bal, y_train = sm.fit_resample(arr, y_train.ravel())\n",
    "    print(f\"After OverSampling, the shape of train_X: {x_train_bal.shape}\")\n",
    "    print(f\"After OverSampling, the shape of train_y: {y_train.shape}\")\n",
    "    print(f\"After OverSampling, counts of label '1': {sum(y_train == 1)}\")\n",
    "    print(f\"After OverSampling, counts of label '0': {sum(y_train == 0)}\")\n",
    "    x_train = numpy.reshape(x_train_bal, (-1, x_train.shape[1], x_train.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "        MAX_VOCAB_SIZE,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stacked_lstm():\n",
    "    document_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedding_sequences =embedding_layer(document_input)\n",
    "    x = LSTM(12, return_sequences=True)(embedding_sequences)\n",
    "    x = LSTM(12)(x)\n",
    "    doc_model = Model(document_input, x)\n",
    "    input_docs = Input(\n",
    "                shape=(TIME_STEP, MAX_SEQUENCE_LENGTH), name=\"input_docs\", dtype=\"int32\"\n",
    "            )\n",
    "\n",
    "    x = TimeDistributed(doc_model, name=\"token_embedding_model\")(input_docs)\n",
    "    x = LSTM(12)(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(input_docs, outputs)\n",
    "\n",
    "    sgd = SGD(lr=LR, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "    model.compile(\n",
    "                loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"]\n",
    "            )\n",
    "    model.summary()\n",
    "    model.fit(x_train, y_train,batch_size=BATCH_SIZE,\n",
    "            epochs=NUM_EPOCHS, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm():\n",
    "    document_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedding_sequences =embedding_layer(document_input)\n",
    "    x = LSTM(12)(embedding_sequences)\n",
    "    doc_model = Model(document_input, x)\n",
    "    input_docs = Input(\n",
    "                shape=(TIME_STEP, MAX_SEQUENCE_LENGTH), name=\"input_docs\", dtype=\"int32\"\n",
    "            )\n",
    "\n",
    "    x = TimeDistributed(doc_model, name=\"token_embedding_model\")(input_docs)\n",
    "    x = LSTM(12)(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(input_docs, outputs)\n",
    "\n",
    "    sgd = SGD(lr=LR, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "    model.compile(\n",
    "                loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"]\n",
    "            )\n",
    "    model.summary()\n",
    "    model.fit(x_train, y_train,batch_size=BATCH_SIZE,\n",
    "            epochs=NUM_EPOCHS,validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn():\n",
    "    document_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedding_sequences = embedding_layer(document_input)\n",
    "\n",
    "    x = Conv1D(filters=300, kernel_size=5, padding=\"valid\")(embedding_sequences)\n",
    "    doc_model = Model(document_input, x)\n",
    "\n",
    "    input_docs = Input(\n",
    "            shape=(TIME_STEP, MAX_SEQUENCE_LENGTH), name=\"input_docs\", dtype=\"int32\"\n",
    "        )\n",
    "\n",
    "    x = TimeDistributed(doc_model, name=\"token_embedding_model\")(input_docs)\n",
    "    x = Conv1D(filters=300, kernel_size=5, padding=\"valid\")(x)\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(input_docs, outputs)\n",
    "\n",
    "    opt = tensorflow.keras.optimizers.Adam(learning_rate=LR, beta_1=0.5, beta_2=0.999)\n",
    "    model.compile(\n",
    "                loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n",
    "            )\n",
    "    model.summary()\n",
    "    model.fit(x_train, y_train,batch_size=BATCH_SIZE,\n",
    "            epochs=NUM_EPOCHS,validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_docs (InputLayer)     [(None, 30, 200)]         0         \n",
      "                                                                 \n",
      " token_embedding_model (Time  (None, 30, 12)           3016224   \n",
      " Distributed)                                                    \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 12)                1200      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,017,437\n",
      "Trainable params: 17,437\n",
      "Non-trainable params: 3,000,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "224/224 [==============================] - 288s 1s/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.8338\n",
      "Epoch 2/5\n",
      "224/224 [==============================] - 267s 1s/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.8338\n",
      "Epoch 3/5\n",
      "224/224 [==============================] - 264s 1s/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.8338\n",
      "Epoch 4/5\n",
      "  5/224 [..............................] - ETA: 4:02 - loss: 0.0000e+00 - accuracy: 0.4781"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aaron\\Desktop\\Longitudinal\\Run_Models.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/aaron/Desktop/Longitudinal/Run_Models.ipynb#ch0000008?line=0'>1</a>\u001b[0m \u001b[39m# run_cnn()\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aaron/Desktop/Longitudinal/Run_Models.ipynb#ch0000008?line=1'>2</a>\u001b[0m run_stacked_lstm()\n",
      "\u001b[1;32mc:\\Users\\aaron\\Desktop\\Longitudinal\\Run_Models.ipynb Cell 9\u001b[0m in \u001b[0;36mrun_stacked_lstm\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/aaron/Desktop/Longitudinal/Run_Models.ipynb#ch0000008?line=17'>18</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/aaron/Desktop/Longitudinal/Run_Models.ipynb#ch0000008?line=18'>19</a>\u001b[0m             loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m, optimizer\u001b[39m=\u001b[39msgd, metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/aaron/Desktop/Longitudinal/Run_Models.ipynb#ch0000008?line=19'>20</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/aaron/Desktop/Longitudinal/Run_Models.ipynb#ch0000008?line=20'>21</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aaron/Desktop/Longitudinal/Run_Models.ipynb#ch0000008?line=21'>22</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train,batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE,\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/aaron/Desktop/Longitudinal/Run_Models.ipynb#ch0000008?line=22'>23</a>\u001b[0m         epochs\u001b[39m=\u001b[39;49mNUM_EPOCHS, validation_data\u001b[39m=\u001b[39;49m(x_val, y_val))\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run_cnn()\n",
    "run_stacked_lstm()\n",
    "# run_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0\n",
      "vegas bonsoir blk livecam [UNK] [UNK] openly reportedly [UNK] econo provides [UNK] [UNK] luther [UNK] [UNK] threw obey left [UNK] [UNK] [UNK] serving [UNK] relaxed [UNK] [UNK] idiots canoe [UNK] ctrl [UNK] [UNK] slate [UNK] [UNK] robust inline [UNK] wall backyard ntsc [UNK] instinct icelandic cbc [UNK] [UNK] molding gdp [UNK] [UNK] zinc tional [UNK] [UNK] patio tal wright oui astra acapulco [UNK] [UNK] pledge [UNK] lamborghini plantronics munich corrupted [UNK] [UNK] [UNK] [UNK] [UNK] lithuania [UNK] stroller whole account stellar [UNK] reed [UNK] [UNK] [UNK] bonjour pertinent whale [UNK] signing tin [UNK] gly largo cher                                                                                                         Label 0\n",
      "nottinghamshire [UNK] [UNK] [UNK] echoes [UNK] sexy [UNK] [UNK] [UNK] [UNK] [UNK] decoder [UNK] monthly [UNK] [UNK] glazed obligated [UNK] [UNK] [UNK] accept ban node [UNK] [UNK] [UNK] homeland [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] solutions injunction meeting bool terminator porno monk [UNK] [UNK] avatar bernard villa [UNK] [UNK] minimum encode msgstr environmental stopped [UNK] ambition [UNK] na decatur [UNK] [UNK] talents sinus [UNK] [UNK] artificial share shouting [UNK] [UNK] garbage verbal scouting roche cs worst [UNK] [UNK] [UNK] doug marshall [UNK] rests [UNK] alps silk [UNK] qualifying [UNK] [UNK] [UNK] installer [UNK] [UNK] males [UNK] panel jigsaw [UNK] slater metaphor [UNK] [UNK]                                                                                                  Label 0\n",
      "bonsoir [UNK] contractual supplier [UNK] [UNK] [UNK] [UNK] zoom arises [UNK] wishing convincing testified spyware [UNK] [UNK] jackson [UNK] meanings [UNK] [UNK] argentine [UNK] salaries camera truly kerr [UNK] [UNK] printprinter [UNK] pavilion informed [UNK] [UNK] detroit evil [UNK] resulted vines trash fats [UNK] compiling [UNK] [UNK] comics belle bear destruction incumbent [UNK] [UNK] theme [UNK] [UNK] [UNK] [UNK] [UNK] crying selections clustering [UNK] creating compilation [UNK] [UNK] curve [UNK] kb ti gandhi [UNK] [UNK] [UNK] customs trap [UNK] scratches [UNK] bonjour foreclosure collar [UNK] glen peggy sin mixer [UNK] interpro blush [UNK] altering kitts swingers lace adequacy [UNK] [UNK] cbc [UNK] [UNK] pw gpo [UNK] [UNK]                                                                                              Label 0\n",
      "cutlery supplier property node [UNK] [UNK] flowering started [UNK] [UNK] enclosures mfg gsa [UNK] queer gowns ample mainframe [UNK] quoting [UNK] competitor flashers [UNK] [UNK] glove agri [UNK] [UNK] [UNK] modification pregnant expedition [UNK] notorious [UNK] [UNK] [UNK] riding mayen [UNK] [UNK] knees [UNK] [UNK] obligations multiple [UNK] advised reject stacked [UNK] publication [UNK] cost obstetrics development semiconductor dynamics [UNK] [UNK] [UNK] [UNK] big valuable advertising selling pirate [UNK] spear [UNK] [UNK] specialize align improvement volunteering [UNK] retain withdraw [UNK] [UNK] reminder [UNK] [UNK] [UNK] [UNK] [UNK] lithuania greenberg plays [UNK] [UNK] shown [UNK] [UNK] erin [UNK] [UNK] dag argue bernard                                                                                                    Label 0\n",
      "[UNK] [UNK] [UNK] seite [UNK] numbers bonsoir [UNK] [UNK] [UNK] leasing inlog fille [UNK] wishing excursion squadron [UNK] commentsblog kisses [UNK] [UNK] forget smooth tes adjusted italy [UNK] documents [UNK] [UNK] [UNK] [UNK] workstation elliot great [UNK] closing [UNK] nerves store [UNK] [UNK] [UNK] [UNK] [UNK] marks [UNK] [UNK] debian [UNK] breakers [UNK] continuum [UNK] [UNK] [UNK] [UNK] medical rollover homme [UNK] [UNK] [UNK] [UNK] australians [UNK] pogo ride [UNK] monmouth domestic have ogg [UNK] staying [UNK] acta communism [UNK] icons [UNK] million [UNK] [UNK] totaling [UNK] search obligations [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] merry flooded [UNK] layers [UNK] [UNK] righteousness firearm                                                                                                  Label 0\n",
      "[UNK] steep basis [UNK] [UNK] learner stl chromium cope belt andale [UNK] [UNK] [UNK] lilies [UNK] [UNK] [UNK] [UNK] mandy conformity fatigue pieces conferred [UNK] inheritance [UNK] munich [UNK] fta practices [UNK] [UNK] [UNK] [UNK] gandhi [UNK] apps grandfather [UNK] twice initialize [UNK] [UNK] [UNK] incorporating [UNK] [UNK] [UNK] reject dirty ansi extend digestive foto [UNK] [UNK] eliminate acapulco antigua head [UNK] [UNK] [UNK] [UNK] especially jolie friedman [UNK] component comply [UNK] [UNK] qty oral munich [UNK] [UNK] sendmail [UNK] [UNK] upright [UNK] enom debian [UNK] [UNK] immortal hour vicki stocks mailman qtr trading sonny [UNK] improve [UNK] [UNK] healthcare imac delle sonny britain [UNK] nominal [UNK] princeton                                                                                             Label 0\n",
      "[UNK] [UNK] fprintf [UNK] bucket [UNK] boob [UNK] [UNK] burners paper dressed site impressum [UNK] willis editor easily archaeological freeman hwy [UNK] cheerleader topaz tying [UNK] [UNK] [UNK] mojo [UNK] [UNK] [UNK] bladder [UNK] close [UNK] stocks alexa [UNK] biking angelina [UNK] sealing [UNK] manga [UNK] colony astm [UNK] pedal [UNK] landmark [UNK] [UNK] chassis ultimate [UNK] amber [UNK] publisher [UNK] pvt aka [UNK] [UNK] daring successes [UNK] riddle [UNK] panda [UNK] [UNK] [UNK] [UNK] [UNK] premiere executive strands peter hart photography karate streets [UNK] [UNK] [UNK] [UNK] assuming bravo apprenticeship [UNK] alignment [UNK] [UNK] actors [UNK] [UNK] henry                                                                                                      Label 0\n",
      "blind rewarded [UNK] powers applicable [UNK] adopting lancashire [UNK] [UNK] brushed shaded [UNK] reps [UNK] allowances dentists [UNK] joshua gentleman wyoming [UNK] [UNK] [UNK] [UNK] george signup organs [UNK] [UNK] [UNK] compilers [UNK] impartial interact [UNK] themes [UNK] [UNK] flown [UNK] popup cruising swinging domino [UNK] [UNK] swingers improvement [UNK] [UNK] [UNK] parks [UNK] [UNK] [UNK] bureaucracy [UNK] [UNK] burned trimmed apples [UNK] [UNK] yoga yet [UNK] [UNK] [UNK] offended [UNK] [UNK] [UNK] converters choice [UNK] anterior [UNK] allegra santa lulu multiple cpt pipeline [UNK] [UNK] defining seiko jolly technological sca audited rhetoric [UNK] rossi                                                                                                          Label 0\n",
      "specialty [UNK] evelyn porno [UNK] choral [UNK] cdn [UNK] [UNK] ob [UNK] scams [UNK] affirmative automate [UNK] lyrics [UNK] dataset tu townsend [UNK] [UNK] bbs [UNK] [UNK] [UNK] [UNK] [UNK] raises headers [UNK] [UNK] [UNK] [UNK] giorgio interiors java nas dlp kazaa wheat [UNK] [UNK] [UNK] sensual [UNK] [UNK] outrageous [UNK] tribute search issuing [UNK] returns aligned premature [UNK] bonsoir [UNK] [UNK] [UNK] [UNK] religion prominent class sources [UNK] fisherman skate [UNK] [UNK] [UNK] [UNK] [UNK] ankle y subs belle [UNK] blk [UNK] bloomberg removing assist consisted arabs [UNK] [UNK] exscuezez moi uruguay escorts [UNK] [UNK] [UNK] barbie                                                                                                       Label 0\n",
      "square [UNK] spreadsheets galaxy [UNK] reverse peterson [UNK] [UNK] [UNK] presbyterian [UNK] [UNK] vine [UNK] [UNK] ax solutions [UNK] [UNK] enjoying spoke [UNK] sheila injured [UNK] keypad [UNK] [UNK] composition [UNK] [UNK] willis [UNK] fact [UNK] encouraged disappointed [UNK] [UNK] jm bus headphone [UNK] hairy chorus rough [UNK] marvin [UNK] [UNK] asserts [UNK] [UNK] assembly bit allocated [UNK] youngest ldap [UNK] [UNK] jail [UNK] [UNK] zen institutional fighter [UNK] persia replaces [UNK] poe [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] gemstone andersen [UNK] carpets [UNK] listened [UNK] [UNK] ebony od amg [UNK] console [UNK] [UNK] [UNK] [UNK] pouring rdf                                                                                                     Label 1\n",
      "[UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] faithful [UNK] [UNK] fre echo za lynne somerset mon capitol partners unesco [UNK] hoc my [UNK] segment [UNK] party [UNK] [UNK] evolve jill wanting questioning racist platform positively pdf prejudice proceeding neglected [UNK] [UNK] [UNK] term verified [UNK] [UNK] [UNK] [UNK] [UNK] marvelous belle [UNK] memorabilia boston underwear villa raising [UNK] [UNK] dressed [UNK] [UNK] [UNK] scientist [UNK] [UNK] [UNK] panama [UNK] rarely ar leverage refresh [UNK] [UNK] politically prizes [UNK] retardation tomorrow [UNK] temps [UNK] convergence [UNK] air [UNK] johnston [UNK] rbi haiti [UNK] mbps gzip bonjour relieved [UNK] [UNK]                                                                                                       Label 1\n",
      "[UNK] belong anal [UNK] [UNK] safely equals [UNK] [UNK] sle [UNK] emp [UNK] [UNK] [UNK] flying wrought backpacking targets bankers [UNK] tick [UNK] [UNK] protected norwood fbi novell distributors [UNK] walmart websites [UNK] catch [UNK] enom [UNK] [UNK] drives position [UNK] desktops immersion [UNK] furnishing [UNK] [UNK] soprano rachel downstream builder [UNK] slowing [UNK] [UNK] plano cq [UNK] humanitarian text [UNK] [UNK] [UNK] variables sedan [UNK] knocking bonjour [UNK] crossed ought fille [UNK] [UNK] hasbro merging drawer [UNK] retrieve assembly titanic low [UNK] [UNK] [UNK] [UNK] finishing [UNK] discount [UNK] [UNK] shoe petite [UNK] sioux lightning belle ryan mature countrywide ultra fairies                                                                                                   Label 1\n",
      "[UNK] green [UNK] fille oui roses [UNK] [UNK] eating draws cockpit [UNK] incorporates [UNK] indicates [UNK] bombers socio fore [UNK] lace [UNK] elapsed [UNK] [UNK] dropped lecture [UNK] rms forward goals hay kinds premiums [UNK] speech [UNK] product incorporating stewardship bbb [UNK] lyric [UNK] communion still homme claw brother [UNK] [UNK] recovering ill talbot scripts [UNK] feat barker tipping [UNK] undoubtedly mom enrolled gam price [UNK] [UNK] [UNK] [UNK] [UNK] milfseeker reproductions authenticated [UNK] [UNK] [UNK] [UNK] malpractice [UNK] inhibitor [UNK] [UNK] [UNK] [UNK] eli [UNK] [UNK] [UNK] [UNK] cf [UNK] [UNK] [UNK] [UNK] [UNK] hath eine decorative [UNK] enzyme [UNK] birmingham [UNK] [UNK] retirement jazz hast osama                                                                                             Label 1\n",
      "caldwell townsend woodward sdl [UNK] undergraduate [UNK] [UNK] maybe fta [UNK] burma fta hunks [UNK] [UNK] [UNK] [UNK] [UNK] injured lob [UNK] footprint detroit therapists plated bac camps acid bonsoir turf displaced [UNK] refrigeration silk satisfy [UNK] [UNK] hoffman [UNK] [UNK] [UNK] [UNK] analysts [UNK] [UNK] [UNK] kart manually shaded [UNK] term wheeled [UNK] discrepancies fires ci [UNK] mechanisms [UNK] luckily [UNK] jill [UNK] [UNK] [UNK] hallmark collectors [UNK] [UNK] associated nsf hostage [UNK] forbidden [UNK] [UNK] [UNK] [UNK] [UNK] tj temps aged [UNK] gone apis [UNK] [UNK] [UNK] rsvp [UNK] [UNK] [UNK] [UNK] pot [UNK] study [UNK] tails [UNK] [UNK] enya [UNK] [UNK] bonjour incest [UNK] dip                                                                                             Label 1\n",
      "submit [UNK] [UNK] [UNK] robust [UNK] [UNK] [UNK] cups dances [UNK] [UNK] tesco religion vibrant capacity hamburg [UNK] [UNK] communal [UNK] tying [UNK] bias [UNK] competency twentieth [UNK] village [UNK] figurines [UNK] paging [UNK] engagement astm wax oncology [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] warranted [UNK] [UNK] feared homme [UNK] [UNK] [UNK] earring [UNK] dion [UNK] [UNK] temps popular [UNK] [UNK] [UNK] cmp acceptance dear [UNK] mail qb [UNK] [UNK] feb inherently [UNK] [UNK] [UNK] [UNK] [UNK] customise [UNK] bonjour [UNK] [UNK] ultra immunity [UNK] men winters [UNK] f [UNK] motions [UNK] [UNK]                                                                                                         Label 1\n",
      "[UNK] childhood chambers mounted principally packers [UNK] [UNK] [UNK] [UNK] [UNK] editorials luncheon attitudes transitional bradley [UNK] [UNK] [UNK] [UNK] affordable forests bulldog [UNK] [UNK] vacancy jigsaw prog [UNK] indy [UNK] oncology [UNK] simply [UNK] maya dram [UNK] [UNK] [UNK] [UNK] matches [UNK] [UNK] pharmacists [UNK] peggy [UNK] [UNK] [UNK] [UNK] invaluable [UNK] ronald obstetrics word exscuezez moi quoting rather [UNK] signage fille [UNK] temps timeshares luther salsa fun [UNK] [UNK] ville confrontation marie qtr [UNK] lavigne comforter [UNK] [UNK] [UNK] rogers optimizing connectors [UNK] rivers strands odessa [UNK] [UNK] theatres athletics dire sights capitol kool eve [UNK] overhead [UNK]                                                                                                     Label 1\n",
      "coordinator [UNK] results beethoven communist np sober acted [UNK] technical [UNK] [UNK] they blamed results [UNK] [UNK] federally [UNK] [UNK] pie headphone [UNK] [UNK] elevated hl [UNK] tickboxes [UNK] [UNK] um live [UNK] [UNK] chap [UNK] alexander [UNK] ethical [UNK] calculator housed man folded rebellion discography homme [UNK] gilbert turns [UNK] [UNK] abusive [UNK] priorities aperture [UNK] launch piss [UNK] spots embarrassing belle [UNK] [UNK] [UNK] fre commodities [UNK] [UNK] [UNK] zaire [UNK] [UNK] dcs [UNK] [UNK] october [UNK] [UNK] [UNK] peace [UNK] kh [UNK] [UNK] [UNK] enjoying [UNK] precisely [UNK] mysimon [UNK] [UNK] steals upward chop sl lynn fille [UNK] [UNK]                                                                                                   Label 1\n",
      "counting [UNK] [UNK] [UNK] [UNK] barbecue sportswear [UNK] [UNK] [UNK] [UNK] [UNK] offset [UNK] slap elephants carbohydrate funky [UNK] le faucets redirect [UNK] tribune [UNK] ssl [UNK] reactive [UNK] mh [UNK] [UNK] superpages [UNK] chambers inclusion panty burst ribosomal [UNK] pavement hz [UNK] [UNK] tidy [UNK] weigh washer [UNK] bonjour bright ribbons [UNK] interracial layers foam nyse [UNK] [UNK] seperate [UNK] [UNK] regents [UNK] [UNK] bounty consulate lid [UNK] [UNK] amt [UNK] [UNK] slower [UNK] [UNK] desks telecoms standardized humanitarian letras [UNK] [UNK] [UNK] oui [UNK] [UNK] corresponding [UNK] diarrhea [UNK] [UNK] pricerunner [UNK] diamond hatch [UNK] nm bonsoir billie [UNK] mas [UNK] duct                                                                                                 Label 1\n",
      "consider norwood [UNK] correct emacs silica telecom [UNK] maya sediment blacks mar [UNK] rinse bollywood [UNK] [UNK] kc [UNK] coll gnd along ch inks sunrise [UNK] [UNK] contested collects sheep [UNK] trying specially renal sys [UNK] marijuana [UNK] [UNK] [UNK] [UNK] valuable [UNK] goblet [UNK] older [UNK] [UNK] bonjour [UNK] spite [UNK] [UNK] altering [UNK] [UNK] [UNK] insect cajun assembler [UNK] lifting [UNK] differentiation stab variant ashanti artisan [UNK] [UNK] [UNK] highlighted presley american rare appreciate stools [UNK] [UNK] hu framing pioneer [UNK] skincare recognize gladly belle [UNK] wyatt kept [UNK] [UNK] sonny summarizes [UNK] funeral garments iso schmidt nearest hans [UNK] fille                                                                                                  Label 1\n",
      "slowed ordinances cbd [UNK] [UNK] clearwater filing [UNK] judge harmless oclc [UNK] [UNK] [UNK] [UNK] [UNK] drafts [UNK] [UNK] [UNK] [UNK] [UNK] remove documents dillon utensils disappeared scr temps [UNK] [UNK] cher [UNK] [UNK] [UNK] [UNK] zion taxpayers [UNK] ua vivo voyager [UNK] [UNK] [UNK] universities cathy asin kc gsm kelly [UNK] observers hogtied [UNK] [UNK] [UNK] [UNK] [UNK] craig rj [UNK] communism sgh stationery [UNK] [UNK] economists [UNK] amy organised [UNK] blast [UNK] [UNK] homme [UNK] fresno highlighted heinz [UNK] stokes respiratory robots clothes windows [UNK] yours [UNK] [UNK] [UNK] [UNK] alleged [UNK] em upload govern fille [UNK] saxophone [UNK] [UNK] speaks [UNK] [UNK] tawnee                                                                                               "
     ]
    }
   ],
   "source": [
    "with open(VOCAB_SAVE_FILE, \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "norevisits = numpy.where(y_train == 0)\n",
    "norevisits_indexes = norevisits[0][0:10]\n",
    "revisits = numpy.where(y_train == 1)\n",
    "revisits_indexes = revisits[0][0:10]\n",
    "# print(result)\n",
    "\n",
    "for index, value in enumerate(norevisits_indexes):\n",
    "    print(f\"Label {y_train[value]}\")\n",
    "    sample = x_train[value]\n",
    "    EHR = sample[TIME_STEP - 1]\n",
    "    for word in EHR:\n",
    "        print(vocab[word], end = ' ')\n",
    "        \n",
    "for index, value in enumerate(revisits_indexes):\n",
    "    print(f\"Label {y_train[value]}\")\n",
    "    sample = x_train[value]\n",
    "    EHR = sample[TIME_STEP - 1]\n",
    "    for word in EHR:\n",
    "        print(vocab[word], end = ' ')\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
